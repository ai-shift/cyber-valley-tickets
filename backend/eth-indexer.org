:PROPERTIES:
:GPTEL_MODEL: gemini-2.5-flash-preview-04-17
:GPTEL_BACKEND: Gemini
:GPTEL_SYSTEM: You are a large language model living in Emacs and a helpful assistant. Respond concisely.
:GPTEL_BOUNDS: ((response (453 41479) (41518 43076) (43136 47312) (47373 95953) (96069 140275)))
:END:

*** Write and web3 indexer ad a django manage command which will listen to all events from provided smart contracts and save required data with provided models

#+begin_src python
# app_name/management/commands/index_contracts.py

import time
import json
import os
import sys
from django.core.management.base import BaseCommand, CommandError
from django.db import transaction, IntegrityError
from django.conf import settings
from web3 import Web3
# from web3.middleware import geth_poa_middleware # Uncomment if connecting to PoA networks like Polygon
import json
import os
import time
from datetime import datetime
from django.contrib.auth import get_user_model

# Assuming your models are in an app named 'core'
# Update this import path if your models are elsewhere
try:
    from core.models import CyberValleyUser, EventPlace, Event, Ticket, Notification, LastProcessedBlock
except ImportError:
    raise CommandError("Could not import models. Make sure models.py exists in your app and the app is in INSTALLED_APPS.")


# Install ipfs-cid library: pip install ipfs-cid multicodec
from cid import make_cid
import multicodec

User = get_user_model()

# --- Helper function to encode Multihash components to CID ---
def multihash_to_cid_v1_raw(digest_bytes: bytes, hash_function_code: int, size: int) -> str | None:
    """Encodes multihash components into an IPFS CIDv1 string (raw codec)."""
    try:
        # Get hash function name from code
        hash_codec_info = multicodec.get_codec(code=hash_function_code)
        hash_func_name = hash_codec_info['name']

        # CIDv1 requires a content codec. Assuming 'raw' (0x55) for generic data.
        # 0x55 is the code for 'raw'
        cid_codec_info = multicodec.get_codec(code=0x55)
        cid_codec_name = cid_codec_info['name']

        # The hash_value expected by make_cid is the raw hash output bytes, truncated to size
        hash_value_bytes = digest_bytes[:size]

        # Create CIDv1
        cid = make_cid(1, codec=cid_codec_name, hash_function=hash_func_name, hash_value=hash_value_bytes)
        return str(cid)
    except Exception as e:
        # Log or handle the error appropriately
        print(f"Error encoding CID: {e}", file=sys.stderr)
        return None


# --- Event Handler Functions ---
def get_or_create_user(address: str) -> CyberValleyUser:
    """Gets or creates a CyberValleyUser based on address."""
    # Normalize address to checksum address for consistency
    checksum_address = Web3.to_checksum_address(address)
    user, created = User.objects.get_or_create(address=checksum_address)
    # Note: User role syncing from contract (RoleGranted/Revoked) is not implemented here.
    return user

def create_notification(user: CyberValleyUser, title: str, body: str) -> None:
    """Creates a Notification for a user."""
    if user:
        # Check if a similar recent notification exists to avoid spamming?
        # For simplicity, just create it.
        try:
            Notification.objects.create(user=user, title=title, body=body)
            print(f"Created notification for {user.address}: {title}")
        except Exception as e:
            # Log DB errors during notification creation but don't stop indexing
            print(f"Error creating notification for {user.address}: {e}", file=sys.stderr)


# Assuming contract addresses and ABIs are configured in Django settings
# Example in settings.py:
# WEB3_PROVIDER_WS = 'ws://localhost:8546' # Or a provider like Infura/Alchemy
# EVENT_MANAGER_ADDRESS = '0x...'
# EVENT_TICKET_ADDRESS = '0x...'
# EVENT_MANAGER_ABI_PATH = 'path/to/EventManager.json' # Path relative to BASE_DIR
# EVENT_TICKET_ABI_PATH = 'path/to/EventTicket.json' # Path relative to BASE_DIR
# MASTER_ADDRESS = '0x...' # Master address for notifications


def get_contract_abi(abi_path: str) -> list:
    """Loads ABI from a JSON file."""
    # Ensure the path is absolute if needed, or relative to BASE_DIR
    full_abi_path = os.path.join(settings.BASE_DIR, abi_path)
    if not os.path.exists(full_abi_path):
        raise FileNotFoundError(f"ABI file not found at: {full_abi_path}")
    with open(full_abi_path, 'r') as f:
        # Assuming the ABI is directly in the file, not inside { "abi": [...] }
        # Handle standard Hardhat/Truffle artifact format as well
        try:
            abi_data = json.load(f)
            abi = abi_data.get('abi', abi_data)
            if not isinstance(abi, list):
                 raise ValueError("Invalid ABI format: Expected a list or artifact JSON.")
            return abi
        except json.JSONDecodeError:
             raise ValueError(f"Invalid JSON in ABI file: {full_abi_path}")
        except Exception as e:
             raise ValueError(f"Error loading ABI from {full_abi_path}: {e}")


class Command(BaseCommand):
    help = 'Listens to smart contract events and indexes data into the database.'

    def add_arguments(self, parser):
        parser.add_argument(
            '--from-block',
            type=int,
            help='Start syncing from this block number (overrides last saved block).',
            default=None
        )
        parser.add_argument(
            '--no-sync',
            action='store_true',
            help='Skip historical sync and only listen for new events from the latest block.',
        )


    def handle_event_place_available(self, event: dict, w3: Web3):
        args = event['args']
        event_place_id = args['eventPlaceId']
        block_timestamp = w3.eth.get_block(event['blockNumber'])['timestamp']

        try:
            with transaction.atomic():
                # Check if it already exists to avoid IntegrityError on get_or_create PK collision
                if EventPlace.objects.filter(id=event_place_id).exists():
                    self.stdout.write(self.style.WARNING(f"EventPlace {event_place_id} already exists on NewEventPlaceAvailable event. Skipping creation."))
                    return

                event_place = EventPlace.objects.create(
                    id=event_place_id,
                    title=f"Event Place {event_place_id}", # Placeholder title, update if metadata added later
                    max_tickets=args['maxTickets'],
                    min_tickets=args['minTickets'],
                    min_price=args['minPrice'],
                    min_days=args['minDays'],
                    available=True, # Assuming new places are available
                )
                self.stdout.write(self.style.SUCCESS(f"Indexed NewEventPlaceAvailable: Place ID {event_place.id}"))

        except IntegrityError as e:
             self.stdout.write(self.style.ERROR(f"Integrity Error handling NewEventPlaceAvailable (Place ID {event_place_id}): {e}"))
        except Exception as e:
            self.stdout.write(self.style.ERROR(f"Error handling NewEventPlaceAvailable (Place ID {event_place_id}): {e}"))


    def handle_event_place_updated(self, event: dict, w3: Web3):
        args = event['args']
        event_place_id = args['eventPlaceId']
        block_timestamp = w3.eth.get_block(event['blockNumber'])['timestamp']

        try:
            with transaction.atomic():
                event_place = EventPlace.objects.get(id=event_place_id)
                event_place.max_tickets = args['maxTickets']
                event_place.min_tickets = args['minTickets']
                event_place.min_price = args['minPrice']
                event_place.min_days = args['minDays']
                # Note: availability is not updated by this event in the contract
                event_place.save()
                self.stdout.write(self.style.SUCCESS(f"Indexed EventPlaceUpdated: Place ID {event_place.id}"))
            except EventPlace.DoesNotExist:
                 self.stdout.write(self.style.ERROR(f"Error: EventPlace {event_place_id} not found for update!"))
            except Exception as e:
                self.stdout.write(self.style.ERROR(f"Error handling EventPlaceUpdated (Place ID {event_place_id}): {e}"))


    def handle_new_event_request(self, event: dict, w3: Web3):
        args = event['args']
        event_contract_id = args['id']
        block_timestamp = w3.eth.get_block(event['blockNumber'])['timestamp']

        try:
            with transaction.atomic():
                creator_user = get_or_create_user(args['creator'])
                try:
                    event_place = EventPlace.objects.get(id=args['eventPlaceId'])
                except EventPlace.DoesNotExist:
                     self.stdout.write(self.style.ERROR(f"Error: EventPlace {args['eventPlaceId']} not found for NewEventRequest (Event ID {event_contract_id})!"))
                     return # Cannot create event without place

                # The Event model doesn't have a `contract_id` field in models.py.
                # It uses Django's default auto-incrementing PK `id`.
                # We need to find the Django Event object that corresponds to the contract event ID.
                # This requires a mapping or assuming the order is the same, which is unsafe.
                # A better approach would be to add `contract_id = models.PositiveIntegerField(unique=True)`
                # to the Event model and use that for lookup.

                # *** Assuming for this code example that Event model has `contract_id` ***
                # *** You NEED to add `contract_id = models.PositiveIntegerField(unique=True)` to your core.models.Event ***

                # Check if event already exists by contract_id to avoid IntegrityError
                if Event.objects.filter(contract_id=event_contract_id).exists():
                    self.stdout.write(self.style.WARNING(f"Event with contract_id {event_contract_id} already exists on NewEventRequest event. Skipping creation."))
                    return

                event_obj = Event.objects.create(
                     contract_id=event_contract_id, # Use contract ID
                     creator=creator_user,
                     place=event_place,
                     ticket_price=args['ticketPrice'],
                     start_date=datetime.fromtimestamp(args['startDate']),
                     days_amount=args['daysAmount'],
                     # Map contract enum (int) to Django choice string
                     status=Event.STATUS_CHOICES.get("submitted", "submitted"),
                     # Title/Description/Image URL are not in contract events, should be added via backend API
                     title=f"Event {event_contract_id}", # Placeholder
                     description="No description available from contract event.", # Placeholder
                     image_url=None, # Placeholder
                     tickets_bought=0, # Initial tickets bought is 0
                     created_at=datetime.fromtimestamp(block_timestamp),
                     updated_at=datetime.fromtimestamp(block_timestamp),
                     # cancel_date is in contract event
                     cancel_date=datetime.fromtimestamp(args['cancelDate']), # Add this field to Django Event model
                )

                # Notification for Master
                master_address = getattr(settings, 'MASTER_ADDRESS', None)
                if master_address:
                     master_user = get_or_create_user(master_address)
                     create_notification(
                          master_user,
                          "New Event Request",
                          f"A new event request (ID: {event_contract_id}) has been submitted by {args['creator']} for place {args['eventPlaceId']}."
                     )

                self.stdout.write(self.style.SUCCESS(f"Indexed NewEventRequest: Event ID {event_contract_id}"))

        except IntegrityError as e:
             self.stdout.write(self.style.ERROR(f"Integrity Error handling NewEventRequest (Event ID {event_contract_id}): {e}"))
        except Exception as e:
            self.stdout.write(self.style.ERROR(f"Error handling NewEventRequest (Event ID {event_contract_id}): {e}"))


    def handle_event_status_changed(self, event: dict, w3: Web3):
        args = event['args']
        event_contract_id = args['eventId']
        block_timestamp = w3.eth.get_block(event['blockNumber'])['timestamp']

        try:
            with transaction.atomic():
                # Assuming the Event model now has 'contract_id'
                event_obj = Event.objects.get(contract_id=event_contract_id)

                # Map contract enum value (integer) to Django choice string
                # EventStatus { Submitted, Approved, Declined, Cancelled, Closed } -> 0, 1, 2, 3, 4
                status_map = {
                    0: "submitted",
                    1: "approved",
                    2: "declined",
                    3: "cancelled",
                    4: "closed",
                }
                new_status_str = status_map.get(args['status'], "unknown")

                if new_status_str == "unknown":
                     self.stdout.write(self.style.ERROR(f"Error: Unknown event status {args['status']} for event {event_contract_id}"))
                     return

                event_obj.status = new_status_str
                event_obj.updated_at = datetime.fromtimestamp(block_timestamp)
                event_obj.save()

                self.stdout.write(self.style.SUCCESS(f"Indexed EventStatusChanged: Event ID {event_contract_id}, Status {new_status_str}"))

                # Send notifications based on status change
                creator_user = event_obj.creator
                event_title = event_obj.title or f"Event {event_obj.contract_id}" # Use title if available

                if new_status_str == "approved":
                    create_notification(
                        creator_user,
                        "Event Approved",
                        f"Your event '{event_title}' has been approved!"
                    )

                elif new_status_str == "declined":
                    create_notification(
                        creator_user,
                        "Event Declined",
                        f"Your event '{event_title}' has been declined."
                    )

                elif new_status_str == "cancelled":
                    # Notify creator and ticket holders
                    create_notification(
                         creator_user,
                         "Event Cancelled",
                         f"Your event '{event_title}' has been cancelled."
                    )
                    # Fetch tickets within the atomic block
                    for ticket in event_obj.tickets.select_related('owner').all():
                        create_notification(
                            ticket.owner,
                            "Event Cancelled",
                            f"The event '{event_title}' you bought a ticket for has been cancelled."
                        )

                elif new_status_str == "closed":
                    # Notify creator and ticket holders
                    create_notification(
                         creator_user,
                         "Event Closed",
                         f"Your event '{event_title}' has been closed."
                    )
                    # Fetch tickets within the atomic block
                    for ticket in event_obj.tickets.select_related('owner').all():
                        create_notification(
                            ticket.owner,
                            "Event Closed",
                            f"The event '{event_title}' you bought a ticket for has been closed."
                        )

        except Event.DoesNotExist:
             self.stdout.write(self.style.ERROR(f"Error: Event {event_contract_id} not found for status change!"))
        except Exception as e:
            self.stdout.write(self.style.ERROR(f"Error handling EventStatusChanged (Event ID {event_contract_id}): {e}"))


    def handle_event_updated(self, event: dict, w3: Web3):
        args = event['args']
        event_contract_id = args['id']
        block_timestamp = w3.eth.get_block(event['blockNumber'])['timestamp']

        try:
            with transaction.atomic():
                # Assuming the Event model now has 'contract_id'
                event_obj = Event.objects.get(contract_id=event_contract_id)
                try:
                     event_place = EventPlace.objects.get(id=args['eventPlaceId'])
                except EventPlace.DoesNotExist:
                     self.stdout.write(self.style.ERROR(f"Error: EventPlace {args['eventPlaceId']} not found for EventUpdated (Event ID {event_contract_id})!"))
                     return # Cannot update event without place

                event_obj.place = event_place
                event_obj.ticket_price = args['ticketPrice']
                event_obj.start_date = datetime.fromtimestamp(args['startDate'])
                event_obj.days_amount = args['daysAmount']
                event_obj.cancel_date = datetime.fromtimestamp(args['cancelDate']) # Add this field to Django Event model
                event_obj.updated_at = datetime.fromtimestamp(block_timestamp)

                # Note: title, description, image_url are not updated by this event.
                # These fields are assumed to be managed off-chain via the backend API.

                event_obj.save()

                self.stdout.write(self.style.SUCCESS(f"Indexed EventUpdated: Event ID {event_contract_id}"))

        except Event.DoesNotExist:
             self.stdout.write(self.style.ERROR(f"Error: Event {event_contract_id} not found for update!"))
        except Exception as e:
            self.stdout.write(self.style.ERROR(f"Error handling EventUpdated (Event ID {event_contract_id}): {e}"))


    def handle_ticket_minted(self, event: dict, w3: Web3):
        args = event['args']
        event_contract_id = args['eventId']
        ticket_token_id = args['ticketId'] # uint256 from contract
        block_timestamp = w3.eth.get_block(event['blockNumber'])['timestamp']

        # The Django Ticket model PK `id` is CharField, mapping to contract `tokenId` (uint256)
        ticket_id_str = str(ticket_token_id)

        try:
            with transaction.atomic():
                owner_user = get_or_create_user(args['owner'])
                try:
                    # Assuming the Event model now has 'contract_id'
                    event_obj = Event.objects.select_related('creator').get(contract_id=event_contract_id)
                except Event.DoesNotExist:
                     self.stdout.write(self.style.ERROR(f"Error: Event {event_contract_id} not found for TicketMinted (Ticket ID {ticket_id_str})!"))
                     return # Cannot create ticket without event

                # Check if ticket already exists by PK (contract tokenId)
                if Ticket.objects.filter(id=ticket_id_str).exists():
                     self.stdout.write(self.style.WARNING(f"Ticket with ID {ticket_id_str} already exists on TicketMinted event. Skipping creation."))
                     return

                # Encode IPFS CID from multihash components
                ipfs_cid_str = multihash_to_cid_v1_raw(args['digest'], args['hashFunction'], args['size'])
                if ipfs_cid_str is None:
                     self.stdout.write(self.style.ERROR(f"Could not encode CID for ticket {ticket_id_str} from event data."))
                     # Decide if this is a fatal error or create ticket without CID

                ticket = Ticket.objects.create(
                     id=ticket_id_str, # Use contract ticketId (tokenId) as Django PK
                     event=event_obj,
                     owner=owner_user,
                     is_redeemed=False, # Newly minted tickets are not redeemed
                     ipfs_cid=ipfs_cid_str, # Save the calculated CID (can be None)
                 )

                # Increment tickets_bought counter on the event
                event_obj.tickets_bought += 1
                event_obj.save()

                self.stdout.write(self.style.SUCCESS(f"Indexed TicketMinted: Ticket ID {ticket.id} for Event {event_obj.contract_id}"))

                # Send notifications
                event_title = event_obj.title or f"Event {event_obj.contract_id}"
                # To Customer
                create_notification(
                     owner_user,
                     "Ticket Purchased",
                     f"You have successfully purchased ticket {ticket.id} for event '{event_title}'."
                )
                # To Creator
                create_notification(
                     event_obj.creator,
                     "Ticket Purchased",
                     f"A ticket has been purchased for your event '{event_title}' by {owner_user.address} (Ticket ID: {ticket.id})."
                )

        except IntegrityError as e:
            self.stdout.write(self.style.ERROR(f"Integrity Error handling TicketMinted (Ticket ID {ticket_id_str}): {e}"))
        except Exception as e:
            self.stdout.write(self.style.ERROR(f"Error handling TicketMinted (Ticket ID {ticket_id_str}): {e}"))


    def handle_ticket_redeemed(self, event: dict, w3: Web3):
        args = event['args']
        ticket_token_id = args['ticketId'] # uint256 from contract

        # The Django Ticket model PK `id` is CharField, mapping to contract `tokenId` (uint256)
        ticket_id_str = str(ticket_token_id)

        try:
            with transaction.atomic():
                # Assuming the Ticket model's PK is the contract ticketId string
                # Use select_related to fetch owner and event in one query for notifications
                ticket = Ticket.objects.select_related('owner', 'event__creator').get(id=ticket_id_str)

                if ticket.is_redeemed:
                     self.stdout.write(self.style.WARNING(f"Ticket {ticket.id} was already redeemed."))
                     return # Avoid redundant updates and notifications

                ticket.is_redeemed = True
                ticket.save()

                self.stdout.write(self.style.SUCCESS(f"Indexed TicketRedeemed: Ticket ID {ticket.id}"))

                # Send notifications
                event_obj = ticket.event
                event_title = event_obj.title or f"Event {event_obj.contract_id}"
                master_address = getattr(settings, 'MASTER_ADDRESS', None)
                master_user = get_or_create_user(master_address) if master_address else None

                # To Customer
                create_notification(
                     ticket.owner,
                     "Ticket Redeemed",
                     f"Your ticket ({ticket.id}) for event '{event_title}' has been redeemed."
                )
                # To Creator
                create_notification(
                     event_obj.creator,
                     "Ticket Redeemed",
                     f"A ticket ({ticket.id}) for your event '{event_title}' has been redeemed."
                )
                # To Master (if configured)
                if master_user:
                     create_notification(
                          master_user,
                          "Ticket Redeemed",
                          f"A ticket ({ticket.id}) for event '{event_title}' has been redeemed by staff."
                     )

            except Ticket.DoesNotExist:
                 self.stdout.write(self.style.ERROR(f"Error: Ticket {ticket_id_str} not found for redemption!"))
            except Exception as e:
                self.stdout.write(self.style.ERROR(f"Error handling TicketRedeemed (Ticket ID {ticket_id_str}): {e}"))

    # TODO: Implement handlers for RoleGranted and RoleRevoked if syncing roles is needed


    def get_last_processed_block(self, start_block_override: int | None) -> int:
         """Gets the last processed block number from DB or uses override."""
         if start_block_override is not None:
              self.stdout.write(f"Using --from-block override: {start_block_override}")
              return start_block_override
         try:
             last_block_entry = LastProcessedBlock.objects.get_or_create(pk=1)[0]
             self.stdout.write(f"Last processed block from DB: {last_block_entry.block_number}")
             return last_block_entry.block_number
         except Exception as e:
              self.stdout.write(self.style.ERROR(f"Error retrieving last processed block from DB: {e}. Starting from block 0."))
              # Fallback to 0 on database error
              return 0

    def save_last_processed_block(self, block_number: int) -> None:
         """Saves the last processed block number to DB."""
         if block_number is None:
             return # Don't save None
         try:
              last_block_entry, created = LastProcessedBlock.objects.get_or_create(pk=1)
              last_block_entry.block_number = block_number
              last_block_entry.save()
              # print(f"Saved last processed block: {block_number}") # Too verbose
         except Exception as e:
              self.stdout.write(self.style.ERROR(f"Error saving last processed block {block_number} to DB: {e}"))


    def process_events(self, events: list[dict], w3: Web3) -> int | None:
        """Processes a list of events, grouped by block."""
        if not events:
            return None # No events to process

        # Sort events first by block number, then by log index within the block
        events.sort(key=lambda x: (x['blockNumber'], x['logIndex']))

        processed_up_to_block = None
        current_block_events = []
        last_block_num = None

        for event in events:
            block_num = event['blockNumber']

            if last_block_num is not None and block_num > last_block_num:
                # Process all events from the previous block before starting the new one
                self._process_block_events(current_block_events, w3)
                # If block processing was successful, update processed block marker
                self.save_last_processed_block(last_block_num)
                processed_up_to_block = last_block_num
                current_block_events = [] # Reset for new block

            current_block_events.append(event)
            last_block_num = block_num

        # Process any remaining events in the last block
        if current_block_events:
            self._process_block_events(current_block_events, w3)
            self.save_last_processed_block(last_block_num)
            processed_up_to_block = last_block_num


        return processed_up_to_block # Return the block number up to which events were processed

    def _process_block_events(self, events: list[dict], w3: Web3):
        """Processes all events within a single block."""
        if not events:
            return
        block_num = events[0]['blockNumber']
        self.stdout.write(f"Processing {len(events)} events in block {block_num}...")
        # Fetch block data once per block if needed by handlers
        try:
            block_data = w3.eth.get_block(block_num)
        except Exception as e:
            self.stdout.write(self.style.ERROR(f"Failed to get block data for block {block_num}: {e}"))
            # Depending on severity, you might want to skip the block or raise an error
            return # Skip processing events for this block if block data unavailable

        for event in events:
            event_name = event.get('event')
            if not event_name:
                 print(f"Skipping event with no name: {event}", file=sys.stderr)
                 continue

            tx_hash_hex = event['transactionHash'].hex()
            self.stdout.write(f"-> Processing event: {event_name} (Tx: {tx_hash_hex}, Log Index: {event['logIndex']})")

            # Add block data to event dict for handlers that need timestamp/block details
            event['block'] = block_data

            try:
                if event_name == 'NewEventPlaceAvailable':
                    self.handle_event_place_available(event, w3)
                elif event_name == 'EventPlaceUpdated':
                    self.handle_event_place_updated(event, w3)
                elif event_name == 'NewEventRequest':
                    self.handle_new_event_request(event, w3)
                elif event_name == 'EventStatusChanged':
                    self.handle_event_status_changed(event, w3)
                elif event_name == 'EventUpdated':
                     self.handle_event_updated(event, w3)
                elif event_name == 'TicketMinted':
                     self.handle_ticket_minted(event, w3)
                elif event_name == 'TicketRedeemed':
                     self.handle_ticket_redeemed(event, w3)
                # Add handlers for RoleGranted/RoleRevoked if syncing roles later
                # elif event_name == 'RoleGranted':
                #     self.handle_role_granted(event, w3)
                # elif event_name == 'RoleRevoked':
                #     self.handle_role_revoked(event, w3)
                else:
                    self.stdout.write(self.style.WARNING(f"Skipping unhandled event: {event_name}"))

            except Exception as e:
                 # Log the error but continue processing other events in the block
                 self.stdout.write(self.style.ERROR(f"Error processing event {event_name} (Tx: {tx_hash_hex}): {e}"))
                 import traceback
                 traceback.print_exc()
            print("-" * 20) # Separator for events within a block


    def handle(self, *args, **options):
        ws_url = getattr(settings, 'WEB3_PROVIDER_WS', None)
        em_address = getattr(settings, 'EVENT_MANAGER_ADDRESS', None)
        et_address = getattr(settings, 'EVENT_TICKET_ADDRESS', None)
        em_abi_path = getattr(settings, 'EVENT_MANAGER_ABI_PATH', None)
        et_abi_path = getattr(settings, 'EVENT_TICKET_ABI_PATH', None)
        poll_interval = getattr(settings, 'INDEXER_POLL_INTERVAL', 10) # Seconds to wait between checking for new blocks
        error_retry_delay = getattr(settings, 'INDEXER_ERROR_RETRY_DELAY', 30) # Seconds to wait after an error

        if not all([ws_url, em_address, et_address, em_abi_path, et_abi_path]):
            raise CommandError("Web3 provider URL (WEB3_PROVIDER_WS), contract addresses (EVENT_MANAGER_ADDRESS, EVENT_TICKET_ADDRESS), and ABI paths (EVENT_MANAGER_ABI_PATH, EVENT_TICKET_ABI_PATH) must be configured in settings.py.")

        try:
            w3 = Web3(Web3.WebsocketProvider(ws_url, websocket_timeout=60)) # Add timeout
            # Add middleware if needed (e.g., PoA for Polygon, infuse with geth_poa_middleware)
            # Example: w3.middleware_onion.inject(geth_poa_middleware, layer=0)

            if not w3.is_connected():
                raise CommandError(f"Could not connect to Web3 provider at {ws_url}")

            self.stdout.write(self.style.SUCCESS(f"Connected to Web3 provider at {ws_url}"))

            try:
                em_abi = get_contract_abi(em_abi_path)
                et_abi = get_contract_abi(et_abi_path)
            except FileNotFoundError as e:
                raise CommandError(f"ABI file not found: {e}")
            except ValueError as e:
                 raise CommandError(f"Invalid ABI file: {e}")


            event_manager_contract = w3.eth.contract(address=Web3.to_checksum_address(em_address), abi=em_abi)
            event_ticket_contract = w3.eth.contract(address=Web3.to_checksum_address(et_address), abi=et_abi)

            self.stdout.write(self.style.SUCCESS("Contract objects created."))

            # --- Historical Sync ---
            last_processed_block = self.get_last_processed_block(options['from_block'])
            current_block = w3.eth.block_number

            if options['no_sync']:
                 self.stdout.write("Historical sync skipped (--no-sync flag).")
                 last_checked_block = current_block
            elif current_block > last_processed_block:
                self.stdout.write(f"Starting historical sync from block {last_processed_block + 1} to {current_block}")
                all_historical_events = []
                start_block = last_processed_block + 1

                # Fetch and collect all historical events first
                try:
                    # get_all_entries might fetch in chunks depending on web3.py version/provider
                    self.stdout.write(f"Fetching EventManager events from {start_block} to {current_block}...")
                    em_events = event_manager_contract.events.get_all_entries(
                        fromBlock=start_block,
                        toBlock=current_block
                    )
                    all_historical_events.extend(em_events)
                    self.stdout.write(f"Fetched {len(em_events)} historical events from EventManager.")
                except Exception as e:
                     self.stdout.write(self.style.ERROR(f"Error fetching historical EventManager events: {e}. Some historical data might be missing."))
                     # Decide if this is fatal or just log and continue

                try:
                    self.stdout.write(f"Fetching EventTicket events from {start_block} to {current_block}...")
                    et_events = event_ticket_contract.events.get_all_entries(
                        fromBlock=start_block,
                        toBlock=current_block
                    )
                    all_historical_events.extend(et_events)
                    self.stdout.write(f"Fetched {len(et_events)} historical events from EventTicket.")
                except Exception as e:
                     self.stdout.write(self.style.ERROR(f"Error fetching historical EventTicket events: {e}. Some historical data might be missing."))
                     # Decide if this is fatal or just log and continue


                # Process historical events block by block
                if all_historical_events:
                     self.stdout.write(f"Processing {len(all_historical_events)} historical events block by block.")
                     processed_block = self.process_events(all_historical_events, w3)
                     if processed_block is not None:
                         self.stdout.write(self.style.SUCCESS(f"Historical sync finished up to block {processed_block}"))
                         last_checked_block = processed_block # Start live polling from here
                     else:
                         self.stdout.write(self.style.WARNING("Historical sync processed no events or encountered errors. Starting live polling from latest block."))
                         last_checked_block = current_block # Start live polling from latest block
                else:
                    self.stdout.write("No historical events found in the range.")
                    last_checked_block = current_block # Start live polling from latest block

            else:
                self.stdout.write(f"Database is already up-to-date (last processed block: {last_processed_block}). Starting live polling.")
                last_checked_block = current_block


            # --- Live Streaming (Polling New Heads) ---
            self.stdout.write(f"Starting live event streaming (polling every {poll_interval} seconds from block {last_checked_block + 1})...")

            while True:
                try:
                    latest_block = w3.eth.block_number
                    if latest_block > last_checked_block:
                        self.stdout.write(f"New block(s) detected: {last_checked_block + 1} to {latest_block}. Fetching events...")
                        new_events = []
                        start_block = last_checked_block + 1

                        # Fetch events for new blocks
                        try:
                            em_new_events = event_manager_contract.events.get_all_entries(
                                fromBlock=start_block,
                                toBlock=latest_block
                            )
                            new_events.extend(em_new_events)
                        except Exception as e:
                             self.stdout.write(self.style.ERROR(f"Error fetching new EventManager events from {start_block} to {latest_block}: {e}. Retrying fetch range."))
                             # On error, retry the fetch range in the next loop iteration
                             time.sleep(error_retry_delay)
                             continue # Skip processing and retry fetch

                        try:
                            et_new_events = event_ticket_contract.events.get_all_entries(
                                fromBlock=start_block,
                                toBlock=latest_block
                            )
                            new_events.extend(et_new_events)
                        except Exception as e:
                             self.stdout.write(self.style.ERROR(f"Error fetching new EventTicket events from {start_block} to {latest_block}: {e}. Retrying fetch range."))
                             # On error, retry the fetch range in the next loop iteration
                             time.sleep(error_retry_delay)
                             continue # Skip processing and retry fetch


                        if new_events:
                            self.stdout.write(f"Found {len(new_events)} new events in blocks {start_block} to {latest_block}.")
                            processed_block = self.process_events(new_events, w3)
                            if processed_block is not None:
                                last_checked_block = processed_block
                                self.stdout.write(self.style.SUCCESS(f"Finished processing up to block {last_checked_block}"))
                            else:
                                 self.stdout.write(self.style.WARNING(f"Failed to process new events fully. Continuing from last successful block {last_checked_block}."))
                                 # ProcessEvents already saves last processed block on success per block
                                 # If process_events returns None, it means no events or critical error occurred,
                                 # last_checked_block remains the same, will retry fetching same range.
                        else:
                            # No events in these blocks, but they are processed implicitly by checking
                            # Save the latest block as processed
                            self.save_last_processed_block(latest_block)
                            last_checked_block = latest_block
                            self.stdout.write(f"No new events in blocks {start_block} to {latest_block}. Marked as processed.")

                    # Wait before checking for new blocks again
                    time.sleep(poll_interval)

                except KeyboardInterrupt:
                    self.stdout.write(self.style.NOTICE("\nIndexer stopped by user."))
                    break
                except Exception as e:
                    self.stdout.write(self.style.ERROR(f"An unexpected error occurred in the polling loop: {e}"))
                    import traceback
                    traceback.print_exc()
                    self.stdout.write(f"Retrying after {error_retry_delay} seconds...")
                    time.sleep(error_retry_delay)


        except CommandError as e:
            self.stdout.write(self.style.ERROR(e))
        except Exception as e:
            self.stdout.write(self.style.ERROR(f"An error occurred during setup or initial sync: {e}"))
            import traceback
            traceback.print_exc()

#+end_src

*Remember to:*

1.  *Update Model:* Add =LastProcessedBlock= model and modify =Event= (add =contract_id=, =cancel_date=) and =Ticket= (add =ipfs_cid=, remove =unique_together=) models in your =core/models.py= (or wherever your models are).
2.  *Run Migrations:* After updating models, run =python manage.py makemigrations core= and =python manage.py migrate=.
3.  *Add to =settings.py=:* Configure =WEB3_PROVIDER_WS=, =EVENT_MANAGER_ADDRESS=, =EVENT_TICKET_ADDRESS=, =EVENT_MANAGER_ABI_PATH=, =ET_ABI_PATH=, and optionally =MASTER_ADDRESS=, =INDEXER_POLL_INTERVAL=, =INDEXER_ERROR_RETRY_DELAY=. =ABI_PATH= should be relative to your project's =BASE_DIR=.
4.  *Place ABIs:* Ensure the =.json= ABI files from your contract compilation are located at the paths specified in =settings.py=.
5.  *Install Dependencies:* =pip install web3 ipfs-cid multicodec=.
6.  *Run Command:* Execute with =python manage.py index_contracts=. Use =python manage.py index_contracts --help= for options like =--from-block= or =--no-sync=.

This indexer uses a simple polling loop based on new blocks to get recent events, which is easier to implement in a synchronous Django management command than a full asynchronous WebSocket listener. For a production system requiring lower latency and higher throughput, a dedicated asynchronous process or service would be more suitable.

*** Define LastProcessedBlock model

#+begin_src python
# core/models.py (or wherever your models are located)

from django.db import models

class LastProcessedBlock(models.Model):
    """
    Stores the block number up to which the contract indexer has processed events.
    There should only be one instance of this model (PK=1).
    """
    # Using PK=1 to ensure there's only ever one row
    id = models.PositiveSmallIntegerField(primary_key=True, default=1)
    block_number = models.PositiveIntegerField(default=0)
    updated_at = models.DateTimeField(auto_now=True)

    def __str__(self):
        return f"Last Processed Block: {self.block_number}"

    # Add constraints or handle in logic to ensure only one instance if default=1 isn't strict enough
    class Meta:
        verbose_name = "Last Processed Block"
        verbose_name_plural = "Last Processed Block" # To avoid pluralization in admin

#+end_src

*Explanation:*

-   =id = models.PositiveSmallIntegerField(primary_key=True, default=1)=: This sets the primary key and defaults it to =1=. By always using =id=1=, you effectively ensure there's only one row in this table, simplifying management.
-   =block_number = models.PositiveIntegerField(default=0)=: Stores the block number as a non-negative integer, defaulting to 0 for the initial state.
-   =updated_at = models.DateTimeField(auto_now=True)=: Automatically updates the timestamp whenever the block number is saved, useful for tracking the last time the indexer ran successfully.
-   =Meta=: Provides a more user-friendly name for the model in the Django admin.

*** Is there a better way to enforce only a single value

Yes, overriding the model's =save= method is a common and more robust way to enforce that only a single instance of the model can exist via the Django ORM.

Here's how you can modify the model:

#+begin_src python
# core/models.py (or wherever your models are located)

from django.db import models, IntegrityError
from django.core.exceptions import ValidationError # Use ValidationError for save method

class LastProcessedBlock(models.Model):
    """
    Stores the block number up to which the contract indexer has processed events.
    Enforces that only a single instance of this model can exist.
    """
    # Keep PK=1 convention for easy retrieval, but enforcement happens in save()
    id = models.PositiveSmallIntegerField(primary_key=True, default=1)
    block_number = models.PositiveIntegerField(default=0)
    updated_at = models.DateTimeField(auto_now=True)

    def __str__(self):
        return f"Last Processed Block: {self.block_number}"

    def clean(self):
        """Validate that no other instance exists."""
        if self.__class__.objects.exists() and self.pk != self.__class__.objects.first().pk:
             # This check is slightly simplified; a more robust check would exclude self by PK
             # If self is new, self.pk is None, exclude(None) does nothing, exists() checks if ANY exist.
             # If self is existing, exclude(self.pk) checks if ANY *other* exist.
             if self.__class__.objects.exclude(pk=self.pk).exists():
                raise ValidationError("Only one instance of LastProcessedBlock is allowed.")


    def save(self, *args, **kwargs):
        """Ensure only one instance is ever saved."""
        # Run clean() for validation
        self.clean()

        # Ensure the PK is 1 before saving, regardless of how it was instantiated
        # This makes 'get_or_create(pk=1)' the standard way to interact.
        if self.pk is None: # Only set default PK if it's a new instance
             self.pk = 1

        # If an instance with pk=1 already exists, this save will update it.
        # If another instance with a different pk was somehow created (e.g., direct DB insert),
        # the clean() method should prevent *saving* it via the ORM after the first one exists.
        # If you try to create a *new* instance (pk=None or pk=1), clean() will catch it if another exists.
        try:
             super().save(*args, **kwargs)
        except IntegrityError:
             # This can happen if save is called concurrently and another process created
             # the instance with pk=1 just before this one.
             # In a high-concurrency scenario, more robust locking or a different
             # singleton pattern might be needed, but for simple indexer state, this is usually sufficient.
             raise ValidationError("Failed to save LastProcessedBlock: Another instance with PK=1 might exist.")


    class Meta:
        verbose_name = "Last Processed Block"
        verbose_name_plural = "Last Processed Block"

#+end_src

/Explanation of the improvements:/

1.  *=clean()= Method:* This is the standard Django place for model-level validation that runs before =save()=. It checks if any /other/ instance of this model already exists in the database using =self.__class__.objects.exclude(pk=self.pk)=. If another instance is found, it raises a =ValidationError=.
2.  *=save()= Method Override:*
    *   Calls =self.clean()= to run the validation logic.
    *   Explicitly sets =self.pk = 1= for new instances (=self.pk is None=). This reinforces that all ORM interactions should target or create the instance with PK 1.
    *   Calls =super().save(*args, **kwargs)= to perform the actual database save.
    *   Includes a basic =try...except IntegrityError= block for saving PK=1, which can help slightly in concurrent scenarios where another process might create the row between your =get_or_create= check and the save, although robust concurrency requires more complex patterns.

This approach provides ORM-level enforcement that complements the =primary_key=True, default=1= convention, making it much harder to accidentally create multiple rows using Django's ORM methods.

*** I want to use sockets api anyway to deliver data ASAP

Okay, integrating with a WebSocket endpoint for real-time event delivery is the standard way to build a fast Web3 indexer. Since Django management commands are typically synchronous and the =web3.py= WebSocket provider is inherently asynchronous, you'll need to use =asyncio= and manage database access carefully using =sync_to_async=.

Here's a refined version of the management command using =asyncio= and polling for new blocks to then fetch logs via =getLogs= (a common pattern that balances real-time with historical sync and resorg handling):

#+begin_src python
# app_name/management/commands/index_contracts.py

import asyncio
import time
import json
import os
import sys
from datetime import datetime
from django.core.management.base import BaseCommand, CommandError
from django.db import transaction, IntegrityError
from django.conf import settings
from django.contrib.auth import get_user_model
from asgiref.sync import sync_to_async # For accessing sync Django ORM in async code
from web3 import Web3
# from web3.middleware import geth_poa_middleware # Uncomment if connecting to PoA networks
from web3.exceptions import ContractLogicError, TransactionNotFound, BlockNotFound

# Assuming your models are in an app named 'core'
try:
    # Add contract_id, cancel_date fields to Event model
    # Add ipfs_cid field to Ticket model
    # Define the LastProcessedBlock model as shown previously
    from core.models import CyberValleyUser, EventPlace, Event, Ticket, Notification, LastProcessedBlock
except ImportError as e:
    raise CommandError(f"Could not import models from core. Make sure core.models.py exists and the app is in INSTALLED_APPS. Error: {e}")


# Install ipfs-cid library: pip install ipfs-cid multicodec
from cid import make_cid
import multicodec

User = get_user_model()

# --- Helper function to encode Multihash components to CID ---
def multihash_to_cid_v1_raw(digest_bytes: bytes, hash_function_code: int, size: int) -> str | None:
    """Encodes multihash components into an IPFS CIDv1 string (raw codec)."""
    try:
        # Get hash function name from code
        # Handle potential KeyError if codec is not found
        try:
            hash_codec_info = multicodec.get_codec(code=hash_function_code)
            hash_func_name = hash_codec_info['name']
        except KeyError:
             print(f"Unknown hash function code: {hash_function_code}", file=sys.stderr)
             return None

        # CIDv1 requires a content codec. Assuming 'raw' (0x55) for generic data.
        try:
             cid_codec_info = multicodec.get_codec(code=0x55) # 0x55 is 'raw'
             cid_codec_name = cid_codec_info['name']
        except KeyError:
            print("Raw multicodec (0x55) not found?", file=sys.stderr)
            return None


        # The hash_value expected by make_cid is the raw hash output bytes, truncated to size
        if len(digest_bytes) < size:
            print(f"Digest bytes length ({len(digest_bytes)}) is less than specified size ({size}).", file=sys.stderr)
            return None
        hash_value_bytes = digest_bytes[:size]

        # Create CIDv1
        cid = make_cid(1, codec=cid_codec_name, hash_function=hash_func_name, hash_value=hash_value_bytes)
        return str(cid)
    except Exception as e:
        # Log or handle the error appropriately
        print(f"Error encoding CID from multihash components: {e}", file=sys.stderr)
        return None


# --- Async Database Access Wrappers ---
# Wrap ORM operations with sync_to_async for use in async handlers
async_get_or_create_user = sync_to_async(get_or_create_user, thread_sensitive=True)
async_create_notification = sync_to_async(create_notification, thread_sensitive=True)

# Wrapper for get_contract_abi
async_get_contract_abi = sync_to_async(get_contract_abi, thread_sensitive=False) # Can run in any thread

# Use sync_to_async for all DB operations within async handlers
async_get_last_processed_block = sync_to_async(LastProcessedBlock.objects.get_or_create, thread_sensitive=True)
async_save_last_processed_block = sync_to_async(LastProcessedBlock.objects.update_or_create, thread_sensitive=True)
async_get_event_place = sync_to_async(EventPlace.objects.get, thread_sensitive=True)
async_create_event_place = sync_to_async(EventPlace.objects.create, thread_sensitive=True)
async_update_event_place = sync_to_async(lambda place: place.save(), thread_sensitive=True) # Helper to await model.save()
async_event_place_exists = sync_to_async(EventPlace.objects.filter(id=None).exists, thread_sensitive=True) # Placeholder, will use filter in handler
async_event_exists = sync_to_async(Event.objects.filter(contract_id=None).exists, thread_sensitive=True) # Placeholder
async_get_event = sync_to_async(Event.objects.get, thread_sensitive=True)
async_create_event = sync_to_async(Event.objects.create, thread_sensitive=True)
async_update_event = sync_to_async(lambda event: event.save(), thread_sensitive=True) # Helper to await model.save()
async_ticket_exists = sync_to_async(Ticket.objects.filter(id=None).exists, thread_sensitive=True) # Placeholder
async_create_ticket = sync_to_async(Ticket.objects.create, thread_sensitive=True)
async_get_ticket = sync_to_async(Ticket.objects.select_related('owner', 'event__creator').get, thread_sensitive=True)
async_update_ticket = sync_to_async(lambda ticket: ticket.save(), thread_sensitive=True) # Helper to await model.save()
async_get_tickets_for_event = sync_to_async(lambda event: list(event.tickets.select_related('owner').all()), thread_sensitive=True) # Fetch related objects within sync wrapper

# --- Event Handler Functions (now async) ---
# Add w3 as an argument if needed inside handlers (e.g., for timestamp)
async def handle_event_place_available(event: dict, w3: Web3):
    args = event['args']
    event_place_id = args['eventPlaceId']
    # block_timestamp = (await sync_to_async(w3.eth.get_block)(event['blockNumber']))['timestamp'] # Get timestamp async if needed

    try:
        # Use sync_to_async wrapped ORM operations
        # Check existence within the same atomic transaction context if possible,
        # or handle IntegrityError on create.
        # For simplicity here, handling IntegrityError on create or checking exists first.
        # Checking exists first reduces noise for duplicates.
        if await async_event_place_exists(id=event_place_id):
            print(f"EventPlace {event_place_id} already exists on NewEventPlaceAvailable event. Skipping creation.", file=sys.stderr)
            return

        # Use sync_to_async transaction management
        @sync_to_async(thread_sensitive=True)
        @transaction.atomic
        def create_place_atomic():
            event_place = EventPlace.objects.create(
                id=event_place_id,
                title=f"Event Place {event_place_id}", # Placeholder title
                max_tickets=args['maxTickets'],
                min_tickets=args['minTickets'],
                min_price=args['minPrice'],
                min_days=args['minDays'],
                available=True, # Assuming new places are available
            )
            return event_place

        event_place = await create_place_atomic()
        print(f"Indexed NewEventPlaceAvailable: Place ID {event_place.id}")

    except IntegrityError as e:
         print(f"Integrity Error handling NewEventPlaceAvailable (Place ID {event_place_id}): {e}", file=sys.stderr)
    except Exception as e:
        print(f"Error handling NewEventPlaceAvailable (Place ID {event_place_id}): {e}", file=sys.stderr)


async def handle_event_place_updated(event: dict, w3: Web3):
    args = event['args']
    event_place_id = args['eventPlaceId']
    # block_timestamp = (await sync_to_async(w3.eth.get_block)(event['blockNumber']))['timestamp']

    try:
        @sync_to_async(thread_sensitive=True)
        @transaction.atomic
        def update_place_atomic():
            event_place = EventPlace.objects.get(id=event_place_id)
            event_place.max_tickets = args['maxTickets']
            event_place.min_tickets = args['minTickets']
            event_place.min_price = args['minPrice']
            event_place.min_days = args['minDays']
            event_place.save() # This save is inside the sync block
            return event_place

        event_place = await update_place_atomic()
        print(f"Indexed EventPlaceUpdated: Place ID {event_place.id}")

    except EventPlace.DoesNotExist:
         print(f"Error: EventPlace {event_place_id} not found for update!", file=sys.stderr)
    except Exception as e:
        print(f"Error handling EventPlaceUpdated (Place ID {event_place_id}): {e}", file=sys.stderr)


async def handle_new_event_request(event: dict, w3: Web3):
    args = event['args']
    event_contract_id = args['id']
    # block_timestamp = (await sync_to_async(w3.eth.get_block)(event['blockNumber']))['timestamp']

    try:
        # Assume Event model has contract_id = models.PositiveIntegerField(unique=True)

        if await async_event_exists(contract_id=event_contract_id):
            print(f"Event with contract_id {event_contract_id} already exists on NewEventRequest event. Skipping creation.", file=sys.stderr)
            return

        # Fetch related objects and create the event atomically
        @sync_to_async(thread_sensitive=True)
        @transaction.atomic
        def create_event_atomic():
            creator_user = get_or_create_user(args['creator'])
            try:
                event_place = EventPlace.objects.get(id=args['eventPlaceId'])
            except EventPlace.DoesNotExist:
                 print(f"Error: EventPlace {args['eventPlaceId']} not found for NewEventRequest (Event ID {event_contract_id})!", file=sys.stderr)
                 return None # Indicate failure to create event

            # Map contract enum (int) to Django choice string
            # EventStatus { Submitted, Approved, Declined, Cancelled, Closed } -> 0, 1, 2, 3, 4
            status_map = {0: "submitted", 1: "approved", 2: "declined", 3: "cancelled", 4: "closed"}
            initial_status = status_map.get(args.get('initialStatus', 0), "submitted") # Assuming initialStatus arg might exist or default

            event_obj = Event.objects.create(
                 contract_id=event_contract_id,
                 creator=creator_user,
                 place=event_place,
                 ticket_price=args['ticketPrice'],
                 start_date=datetime.fromtimestamp(args['startDate']),
                 days_amount=args['daysAmount'],
                 status=initial_status, # Set initial status based on contract state/event
                 # Title/Description/Image URL should be added via backend API after creation
                 title=f"Event {event_contract_id}", # Placeholder
                 description="No description available from contract event.", # Placeholder
                 image_url=None, # Placeholder
                 tickets_bought=0, # Initial tickets bought is 0
                 # created_at=datetime.fromtimestamp(block_timestamp), # Use DB default or remove if not in contract
                 # updated_at=datetime.fromtimestamp(block_timestamp), # Use DB default or remove if not in contract
                 cancel_date=datetime.fromtimestamp(args['cancelDate']),
            )
            return creator_user, event_place, event_obj # Return objects needed for notifications

        result = await create_event_atomic()
        if result is None:
             return # Failed to create event

        creator_user, event_place, event_obj = result

        print(f"Indexed NewEventRequest: Event ID {event_contract_id}")

        # Notification for Master (async)
        master_address = getattr(settings, 'MASTER_ADDRESS', None)
        if master_address:
             master_user = await async_get_or_create_user(master_address)
             await async_create_notification(
                  master_user,
                  "New Event Request",
                  f"A new event request (ID: {event_contract_id}) has been submitted by {args['creator']} for place {args['eventPlaceId']}."
             )

    except IntegrityError as e:
         print(f"Integrity Error handling NewEventRequest (Event ID {event_contract_id}): {e}", file=sys.stderr)
    except Exception as e:
        print(f"Error handling NewEventRequest (Event ID {event_contract_id}): {e}", file=sys.stderr)


async def handle_event_status_changed(event: dict, w3: Web3):
    args = event['args']
    event_contract_id = args['eventId']
    # block_timestamp = (await sync_to_async(w3.eth.get_block)(event['blockNumber']))['timestamp']

    try:
        @sync_to_async(thread_sensitive=True)
        @transaction.atomic
        def update_status_atomic():
            # Assuming the Event model now has 'contract_id'
            event_obj = Event.objects.select_related('creator').get(contract_id=event_contract_id)

            # Map contract enum value (integer) to Django choice string
            # EventStatus { Submitted, Approved, Declined, Cancelled, Closed } -> 0, 1, 2, 3, 4
            status_map = {
                0: "submitted",
                1: "approved",
                2: "declined",
                3: "cancelled",
                4: "closed",
            }
            new_status_str = status_map.get(args['status'], "unknown")

            if new_status_str == "unknown":
                 print(f"Error: Unknown event status {args['status']} for event {event_contract_id}", file=sys.stderr)
                 return None # Indicate failure

            event_obj.status = new_status_str
            # event_obj.updated_at = datetime.fromtimestamp(block_timestamp) # Use DB default or remove if not in contract
            event_obj.save() # Inside sync block

            # Fetch tickets within the atomic block if needed for notifications below
            tickets_to_notify = []
            if new_status_str in ["cancelled", "closed"]:
                 tickets_to_notify = list(event_obj.tickets.select_related('owner').all()) # Fetch related users

            return event_obj, tickets_to_notify, new_status_str # Return objects for notifications

        result = await update_status_atomic()
        if result is None:
             return # Failed to update status

        event_obj, tickets_to_notify, new_status_str = result

        print(f"Indexed EventStatusChanged: Event ID {event_contract_id}, Status {new_status_str}")

        # Send notifications based on status change (async)
        creator_user = event_obj.creator
        event_title = event_obj.title or f"Event {event_obj.contract_id}" # Use title if available

        if new_status_str == "approved":
            await async_create_notification(
                creator_user,
                "Event Approved",
                f"Your event '{event_title}' has been approved!"
            )

        elif new_status_str == "declined":
            await async_create_notification(
                creator_user,
                "Event Declined",
                f"Your event '{event_title}' has been declined."
            )

        elif new_status_str == "cancelled":
            await async_create_notification(
                 creator_user,
                 "Event Cancelled",
                 f"Your event '{event_title}' has been cancelled."
            )
            for ticket in tickets_to_notify:
                await async_create_notification(
                    ticket.owner,
                    "Event Cancelled",
                    f"The event '{event_title}' you bought a ticket for has been cancelled."
                )

        elif new_status_str == "closed":
            await async_create_notification(
                 creator_user,
                 "Event Closed",
                 f"Your event '{event_title}' has been closed."
            )
            for ticket in tickets_to_notify:
                await async_create_notification(
                    ticket.owner,
                    "Event Closed",
                    f"The event '{event_title}' you bought a ticket for has been closed."
                )

    except Event.DoesNotExist:
         print(f"Error: Event {event_contract_id} not found for status change!", file=sys.stderr)
    except Exception as e:
        print(f"Error handling EventStatusChanged (Event ID {event_contract_id}): {e}", file=sys.stderr)


async def handle_event_updated(event: dict, w3: Web3):
    args = event['args']
    event_contract_id = args['id']
    # block_timestamp = (await sync_to_async(w3.eth.get_block)(event['blockNumber']))['timestamp']

    try:
        @sync_to_async(thread_sensitive=True)
        @transaction.atomic
        def update_event_atomic():
            # Assuming the Event model now has 'contract_id'
            event_obj = Event.objects.get(contract_id=event_contract_id)
            try:
                 event_place = EventPlace.objects.get(id=args['eventPlaceId'])
            except EventPlace.DoesNotExist:
                 print(f"Error: EventPlace {args['eventPlaceId']} not found for EventUpdated (Event ID {event_contract_id})!", file=sys.stderr)
                 return None # Cannot update event without place

            event_obj.place = event_place
            event_obj.ticket_price = args['ticketPrice']
            event_obj.start_date = datetime.fromtimestamp(args['startDate'])
            event_obj.days_amount = args['daysAmount']
            event_obj.cancel_date = datetime.fromtimestamp(args['cancelDate'])
            # event_obj.updated_at = datetime.fromtimestamp(block_timestamp) # Use DB default

            # Note: title, description, image_url are not updated by this event.

            event_obj.save() # Inside sync block
            return event_obj

        event_obj = await update_event_atomic()
        if event_obj:
            print(f"Indexed EventUpdated: Event ID {event_contract_id}")

    except Event.DoesNotExist:
         print(f"Error: Event {event_contract_id} not found for update!", file=sys.stderr)
    except Exception as e:
        print(f"Error handling EventUpdated (Event ID {event_contract_id}): {e}", file=sys.stderr)


async def handle_ticket_minted(event: dict, w3: Web3):
    args = event['args']
    event_contract_id = args['eventId']
    ticket_token_id = args['ticketId'] # uint256 from contract
    # block_timestamp = (await sync_to_async(w3.eth.get_block)(event['blockNumber']))['timestamp']

    # The Django Ticket model PK `id` is CharField, mapping to contract `tokenId` (uint256)
    ticket_id_str = str(ticket_token_id)

    try:
        # Encode IPFS CID from multihash components
        ipfs_cid_str = multihash_to_cid_v1_raw(args['digest'], args['hashFunction'], args['size'])
        if ipfs_cid_str is None:
             print(f"Could not encode CID for ticket {ticket_id_str} from event data.", file=sys.stderr)
             # Decide if this is a fatal error or create ticket without CID


        @sync_to_async(thread_sensitive=True)
        @transaction.atomic
        def create_ticket_atomic():
            # Check if ticket already exists by PK (contract tokenId)
            if Ticket.objects.filter(id=ticket_id_str).exists():
                 print(f"Ticket with ID {ticket_id_str} already exists on TicketMinted event. Skipping creation.", file=sys.stderr)
                 return None # Indicate already exists

            owner_user = get_or_create_user(args['owner'])
            try:
                # Assuming the Event model now has 'contract_id' and related creator
                event_obj = Event.objects.select_related('creator').get(contract_id=event_contract_id)
            except Event.DoesNotExist:
                 print(f"Error: Event {event_contract_id} not found for TicketMinted (Ticket ID {ticket_id_str})!", file=sys.stderr)
                 return None # Cannot create ticket without event

            ticket = Ticket.objects.create(
                 id=ticket_id_str, # Use contract ticketId (tokenId) as Django PK
                 event=event_obj,
                 owner=owner_user,
                 is_redeemed=False, # Newly minted tickets are not redeemed
                 ipfs_cid=ipfs_cid_str, # Save the calculated CID (can be None)
             )

            # Increment tickets_bought counter on the event
            event_obj.tickets_bought += 1
            event_obj.save() # Inside sync block

            return ticket, event_obj, owner_user # Return objects for notifications

        result = await create_ticket_atomic()

        if result is None:
            # Could be duplicate or event not found, messages already printed
             return

        ticket, event_obj, owner_user = result

        print(f"Indexed TicketMinted: Ticket ID {ticket.id} for Event {event_obj.contract_id}")

        # Send notifications (async)
        event_title = event_obj.title or f"Event {event_obj.contract_id}"
        # To Customer
        await async_create_notification(
             owner_user,
             "Ticket Purchased",
             f"You have successfully purchased ticket {ticket.id} for event '{event_title}'."
        )
        # To Creator
        await async_create_notification(
             event_obj.creator,
             "Ticket Purchased",
             f"A ticket has been purchased for your event '{event_title}' by {owner_user.address} (Ticket ID: {ticket.id})."
        )

    except IntegrityError as e:
        print(f"Integrity Error handling TicketMinted (Ticket ID {ticket_id_str}): {e}", file=sys.stderr)
    except Exception as e:
        print(f"Error handling TicketMinted (Ticket ID {ticket_id_str}): {e}", file=sys.stderr)


async def handle_ticket_redeemed(event: dict, w3: Web3):
    args = event['args']
    ticket_token_id = args['ticketId'] # uint256 from contract

    # The Django Ticket model PK `id` is CharField, mapping to contract `tokenId` (uint256)
    ticket_id_str = str(ticket_token_id)

    try:
        @sync_to_async(thread_sensitive=True)
        @transaction.atomic
        def redeem_ticket_atomic():
            # Assuming the Ticket model's PK is the contract ticketId string
            # Use select_related to fetch owner and event in one query for notifications
            ticket = Ticket.objects.select_related('owner', 'event__creator').get(id=ticket_id_str)

            if ticket.is_redeemed:
                 print(f"Ticket {ticket.id} was already redeemed.", file=sys.stderr)
                 return None # Indicate already redeemed

            ticket.is_redeemed = True
            ticket.save() # Inside sync block

            return ticket # Return ticket for notifications

        ticket = await redeem_ticket_atomic()
        if result is None:
             return # Already redeemed or ticket not found

        print(f"Indexed TicketRedeemed: Ticket ID {ticket.id}")

        # Send notifications (async)
        event_obj = ticket.event
        event_title = event_obj.title or f"Event {event_obj.contract_id}"
        master_address = getattr(settings, 'MASTER_ADDRESS', None)
        master_user = await async_get_or_create_user(master_address) if master_address else None

        # To Customer
        await async_create_notification(
             ticket.owner,
             "Ticket Redeemed",
             f"Your ticket ({ticket.id}) for event '{event_title}' has been redeemed."
        )
        # To Creator
        await async_create_notification(
             event_obj.creator,
             "Ticket Redeemed",
             f"A ticket ({ticket.id}) for your event '{event_title}' has been redeemed."
        )
        # To Master (if configured)
        if master_user:
             await async_create_notification(
                  master_user,
                  "Ticket Redeemed",
                  f"A ticket ({ticket.id}) for event '{event_title}' has been redeemed by staff."
             )

    except Ticket.DoesNotExist:
         print(f"Error: Ticket {ticket_id_str} not found for redemption!", file=sys.stderr)
    except Exception as e:
        print(f"Error handling TicketRedeemed (Ticket ID {ticket_id_str}): {e}", file=sys.stderr)

# TODO: Implement async handlers for RoleGranted and RoleRevoked


class Command(BaseCommand):
    help = 'Listens to smart contract events and indexes data into the database.'

    def add_arguments(self, parser):
        parser.add_argument(
            '--from-block',
            type=int,
            help='Start syncing from this block number (overrides last saved block).',
            default=None
        )
        parser.add_argument(
            '--no-sync',
            action='store_true',
            help='Skip historical sync and only listen for new events from the latest block.',
        )


    # Main entry point for the command, runs the async function
    def handle(self, *args, **options):
        try:
            asyncio.run(self._async_handle(*args, **options))
        except KeyboardInterrupt:
            self.stdout.write(self.style.NOTICE("\nIndexer stopped by user."))
        except Exception as e:
            self.stdout.write(self.style.ERROR(f"An error occurred during async execution: {e}"))
            import traceback
            traceback.print_exc()


    async def _async_handle(self, *args, **options):
        ws_url = getattr(settings, 'WEB3_PROVIDER_WS', None)
        em_address = getattr(settings, 'EVENT_MANAGER_ADDRESS', None)
        et_address = getattr(settings, 'EVENT_TICKET_ADDRESS', None)
        em_abi_path = getattr(settings, 'EVENT_MANAGER_ABI_PATH', None)
        et_abi_path = getattr(settings, 'EVENT_TICKET_ABI_PATH', None)
        poll_interval = getattr(settings, 'INDEXER_POLL_INTERVAL', 5) # Seconds to wait between checking for new blocks
        error_retry_delay = getattr(settings, 'INDEXER_ERROR_RETRY_DELAY', 15) # Seconds to wait after an error


        if not all([ws_url, em_address, et_address, em_abi_path, et_abi_path]):
            raise CommandError("Web3 provider URL (WEB3_PROVIDER_WS), contract addresses (EVENT_MANAGER_ADDRESS, EVENT_TICKET_ADDRESS), and ABI paths (EVENT_MANAGER_ABI_PATH, EVENT_TICKET_ABI_PATH) must be configured in settings.py.")

        # Use async Web3 provider
        w3 = Web3(Web3.WebsocketProvider(ws_url, websocket_timeout=60))
        # Add async middleware if needed
        # Example: w3.middleware_onion.inject(geth_poa_middleware, layer=0) # Note: Check async compatibility

        # Wait for connection
        print(f"Connecting to Web3 provider at {ws_url}...")
        # A robust way to wait for async connection might involve a loop
        # For simplicity, assuming provider connects quickly or reconnects automatically
        await asyncio.sleep(1) # Give a moment to connect


        if not await sync_to_async(w3.is_connected)(): # w3.is_connected is sync
            raise CommandError(f"Could not connect to Web3 provider at {ws_url}")

        self.stdout.write(self.style.SUCCESS(f"Connected to Web3 provider at {ws_url}"))

        try:
            # Load ABIs asynchronously
            em_abi = await async_get_contract_abi(em_abi_path)
            et_abi = await async_get_contract_abi(et_abi_path)
        except FileNotFoundError as e:
            raise CommandError(f"ABI file not found: {e}")
        except ValueError as e:
             raise CommandError(f"Invalid ABI file: {e}")


        event_manager_contract = w3.eth.contract(address=Web3.to_checksum_address(em_address), abi=em_abi)
        event_ticket_contract = w3.eth.contract(address=Web3.to_checksum_address(et_address), abi=et_abi)

        self.stdout.write(self.style.SUCCESS("Contract objects created."))

        # --- Historical Sync ---
        # Use async DB access
        last_block_entry, created = await async_get_last_processed_block(pk=1)
        last_processed_block = last_block_entry.block_number

        if options['from_block'] is not None:
            last_processed_block = options['from_block'] - 1 # Sync starts FROM this block, so last processed is one before
            self.stdout.write(f"Using --from-block override. Starting sync from block {options['from_block']}")
            # Update DB to reflect the override start block
            await async_save_last_processed_block(pk=1, defaults={'block_number': last_processed_block})
        else:
             self.stdout.write(f"Last processed block from DB: {last_processed_block}")


        current_block = await sync_to_async(w3.eth.block_number)()


        if options['no_sync']:
             self.stdout.write("Historical sync skipped (--no-sync flag).")
             last_checked_block = current_block # Start live polling from current block
             await async_save_last_processed_block(pk=1, defaults={'block_number': last_checked_block}) # Update DB
        elif current_block > last_processed_block:
            self.stdout.write(f"Starting historical sync from block {last_processed_block + 1} to {current_block}")
            sync_start_block = last_processed_block + 1

            # Fetch all historical events in range using getLogs (can handle larger ranges better than get_all_entries sometimes)
            # You might need to chunk this range depending on RPC provider limits
            try:
                 all_historical_events = await self._async_fetch_logs(w3, event_manager_contract, event_ticket_contract, sync_start_block, current_block)
                 self.stdout.write(f"Fetched {len(all_historical_events)} historical events from block {sync_start_block} to {current_block}.")

                 # Process historical events block by block
                 if all_historical_events:
                      self.stdout.write(f"Processing {len(all_historical_events)} historical events block by block.")
                      # _process_events is now async
                      processed_block = await self._async_process_events(all_historical_events, w3)
                      if processed_block is not None:
                          self.stdout.write(self.style.SUCCESS(f"Historical sync finished up to block {processed_block}"))
                          last_checked_block = processed_block # Start live polling from here
                      else:
                          self.stdout.write(self.style.WARNING("Historical sync processed no events or encountered errors. Starting live polling from latest block processed by sync or current block."))
                          # If process_events returned None, last_checked_block should remain the last block successfully saved *within* process_events
                          # If no events processed, it might still be the initial last_processed_block.
                          # Ensure last_checked_block is set correctly even if no events were found in range.
                          # Let's get the last saved block from DB again to be safe.
                          last_block_entry, _ = await async_get_last_processed_block(pk=1)
                          last_checked_block = last_block_entry.block_number
                 else:
                     self.stdout.write("No historical events found in the range.")
                     last_checked_block = current_block # Start live polling from latest block
                     await async_save_last_processed_block(pk=1, defaults={'block_number': last_checked_block}) # Update DB
            except Exception as e:
                 self.stdout.write(self.style.ERROR(f"Error during historical sync: {e}. Starting live polling from last successfully processed block."))
                 import traceback
                 traceback.print_exc()
                 last_block_entry, _ = await async_get_last_processed_block(pk=1)
                 last_checked_block = last_block_entry.block_number


        else:
            self.stdout.write(f"Database is already up-to-date (last processed block: {last_processed_block}). Starting live polling.")
            last_checked_block = current_block # Ensure we poll from latest block if DB is current

        # --- Live Streaming (Polling Latest Block Number) ---
        # Note: A true WebSocket listener subscribes to 'newHeads' and receives blocks
        # as they are mined. This polling approach checks eth.blockNumber periodically
        # and then fetches events using getLogs for any new blocks found.
        # It's simpler to implement in this command structure than a persistent async
        # listener that needs graceful shutdown, reconnection, etc.

        self.stdout.write(f"Starting live event indexing (polling every {poll_interval} seconds from block {last_checked_block + 1})...")

        while True:
            try:
                latest_block = await sync_to_async(w3.eth.block_number)()

                # Process blocks since the last one checked, up to the latest block
                from_block_live = last_checked_block + 1
                to_block_live = latest_block

                if to_block_live >= from_block_live: # Only fetch if there are new blocks
                    self.stdout.write(f"Checking blocks {from_block_live} to {to_block_live} for events...")
                    new_events = await self._async_fetch_logs(w3, event_manager_contract, event_ticket_contract, from_block_live, to_block_live)

                    if new_events:
                        self.stdout.write(f"Found {len(new_events)} new events in blocks {from_block_live} to {to_block_live}.")
                        # Process events block by block
                        processed_block = await self._async_process_events(new_events, w3)
                        if processed_block is not None:
                            last_checked_block = processed_block
                            self.stdout.write(self.style.SUCCESS(f"Finished processing up to block {last_checked_block}"))
                        else:
                             # If processing failed for some reason, keep last_checked_block as is,
                             # The next loop iteration will try to fetch the same range again.
                             self.stdout.write(self.style.WARNING(f"Failed to process new events fully in range {from_block_live}-{to_block_live}. Retrying range in next poll."))
                    else:
                        # No events in these blocks, but they are processed implicitly by checking
                        # Mark the entire range as processed
                        await async_save_last_processed_block(pk=1, defaults={'block_number': to_block_live})
                        last_checked_block = to_block_live
                        self.stdout.write(f"No new events found in blocks {from_block_live} to {to_block_live}. Marked up to {last_checked_block} as processed.")
                else:
                     self.stdout.write(f"No new blocks ({latest_block}) since last check ({last_checked_block}).")


                # Wait before checking for new blocks again
                await asyncio.sleep(poll_interval)

            except KeyboardInterrupt:
                self.stdout.write(self.style.NOTICE("\nIndexer stopping..."))
                break
            except Exception as e:
                self.stdout.write(self.style.ERROR(f"An unexpected error occurred in the polling loop: {e}"))
                import traceback
                traceback.print_exc()
                self.stdout.write(f"Retrying after {error_retry_delay} seconds...")
                await asyncio.sleep(error_retry_delay)

        self.stdout.write(self.style.NOTICE("Indexer loop finished."))


    async def _async_fetch_logs(self, w3, em_contract, et_contract, from_block, to_block):
        """Asynchronously fetches logs from contracts in a given block range."""
        all_events = []
        try:
            # Fetch EventManager events
            # Use contract.events.YourEvent.getLogs() or w3.eth.get_logs()
            # getLogs requires topics for filtering, get_all_entries is simpler but might not be async/available on provider
            # Let's fetch all logs for the contract addresses and parse them
            event_filter = {
                'address': [em_contract.address, et_contract.address],
                'fromBlock': from_block,
                'toBlock': to_block
            }
            # provider.get_logs is the async equivalent if using async provider directly
            # Or use w3.eth.filter and poll (less efficient)
            # For async Web3, contract.get_logs is often the way.
            # Need to instantiate an async filter or use an async helper from web3.py
            # A simpler async way is using w3.eth.get_logs directly if the provider supports it async
            # Let's assume w3.eth.get_logs is awaitable with the async provider
            # Note: Older web3.py versions might not have awaitable get_logs
            # A safer bet might be wrapping sync get_logs in sync_to_async, but it can block the loop
            # Or use w3.eth.filter('logs', filter_params).get_new_entries() in a loop (less efficient)
            # Let's try the awaitable get_logs approach first, it's cleaner if supported.

            # Example using w3.eth.get_logs (requires topics for efficiency)
            # Getting topics requires knowing the event signatures
            # Example: EventManager.events.NewEventPlaceAvailable().hash() for the topic
            # A simpler way for *all* events from specific contracts is the filter method.
            # Let's stick to get_all_entries if it's awaitable, or transition to w3.eth.get_logs with topics.

            # Check if get_all_entries is awaitable (often true for async contracts)
            # If not, we need to get topics and use await w3.eth.get_logs
            # For simplicity, let's try `get_all_entries` first and assume it's async compatible here
            # In a real scenario, verify your web3.py version and provider.

            # It seems contract.events.SomeEvent.get_logs(*args, **kwargs) is the async way
            em_event_types = [
                em_contract.events.NewEventPlaceAvailable,
                em_contract.events.EventPlaceUpdated,
                em_contract.events.NewEventRequest,
                em_contract.events.EventStatusChanged,
                em_contract.events.EventUpdated,
                # Add RoleGranted/RoleRevoked if needed
            ]

            et_event_types = [
                et_contract.events.TicketMinted,
                et_contract.events.TicketRedeemed,
            ]

            for event_type in em_event_types + et_event_types:
                 try:
                      logs = await event_type.get_logs(fromBlock=from_block, toBlock=to_block)
                      all_events.extend(logs)
                      print(f"Fetched {len(logs)} logs for {event_type.event_name} in range {from_block}-{to_block}")
                 except Exception as e:
                      # Log error but continue fetching other events
                      print(f"Error fetching logs for {event_type.event_name} from {from_block} to {to_block}: {e}", file=sys.stderr)
                      # Depending on error, might need more robust handling (e.g., retry smaller range)

        except Exception as e:
             print(f"An error occurred during log fetching for range {from_block}-{to_block}: {e}", file=sys.stderr)
             import traceback
             traceback.print_exc()
             # Re-raise or return empty list based on desired error handling strategy
             return [] # Return empty list on error to avoid crashing

        return all_events


    async def _async_process_events(self, events: list[dict], w3: Web3) -> int | None:
        """Asynchronously processes a list of events, grouped by block."""
        if not events:
            return None # No events to process

        # Sort events first by block number, then by log index within the block
        events.sort(key=lambda x: (x['blockNumber'], x['logIndex']))

        processed_up_to_block = None
        current_block_events = []
        last_block_num = None

        for event in events:
            block_num = event['blockNumber']

            if last_block_num is not None and block_num > last_block_num:
                # Process all events from the previous block before starting the new one
                await self._async_process_block_events(current_block_events, w3)
                # Save processed block number *after* successfully processing all events in it
                # This is crucial for resuming after a crash mid-block processing
                await async_save_last_processed_block(pk=1, defaults={'block_number': last_block_num})
                processed_up_to_block = last_block_num
                current_block_events = [] # Reset for new block

            current_block_events.append(event)
            last_block_num = block_num

        # Process any remaining events in the last block
        if current_block_events:
            await self._async_process_block_events(current_block_events, w3)
            # Save the last block number processed
            await async_save_last_processed_block(pk=1, defaults={'block_number': last_block_num})
            processed_up_to_block = last_block_num


        return processed_up_to_block # Return the block number up to which events were successfully processed

    async def _async_process_block_events(self, events: list[dict], w3: Web3):
        """Asynchronously processes all events within a single block."""
        if not events:
            return
        block_num = events[0]['blockNumber']
        print(f"Processing {len(events)} events in block {block_num}...")
        # Fetch block data once per block if needed by handlers (async)
        block_data = None
        try:
            block_data = await sync_to_async(w3.eth.get_block)(block_num)
        except (BlockNotFound, Exception) as e: # Handle specific Web3 errors
            print(self.style.ERROR(f"Failed to get block data for block {block_num}: {e}"), file=sys.stderr)
            # Depending on severity, you might want to skip the block or raise an error
            # If block data is crucial for handlers, skip this block
            return # Skip processing events for this block if block data unavailable

        for event in events:
            event_name = event.get('event')
            if not event_name:
                 print(f"Skipping event with no name: {event}", file=sys.stderr)
                 continue

            tx_hash_hex = event['transactionHash'].hex()
            print(f"-> Processing event: {event_name} (Tx: {tx_hash_hex}, Log Index: {event['logIndex']})")

            # Add block data to event dict for handlers that need timestamp/block details
            event['block'] = block_data

            try:
                # Call async handlers using await
                if event_name == 'NewEventPlaceAvailable':
                    await handle_event_place_available(event, w3)
                elif event_name == 'EventPlaceUpdated':
                    await handle_event_place_updated(event, w3)
                elif event_name == 'NewEventRequest':
                    await handle_new_event_request(event, w3)
                elif event_name == 'EventStatusChanged':
                    await handle_event_status_changed(event, w3)
                elif event_name == 'EventUpdated':
                     await handle_event_updated(event, w3)
                elif event_name == 'TicketMinted':
                     await handle_ticket_minted(event, w3)
                elif event_name == 'TicketRedeemed':
                     await handle_ticket_redeemed(event, w3)
                # Add handlers for RoleGranted/RoleRevoked if syncing roles later
                # elif event_name == 'RoleGranted':
                #     await handle_role_granted(event, w3)
                # elif event_name == 'RoleRevoked':
                #     await handle_role_revoked(event, w3)
                else:
                    print(f"Skipping unhandled event: {event_name}")

            except Exception as e:
                 # Log the error but continue processing other events in the block
                 print(self.style.ERROR(f"Error processing event {event_name} (Tx: {tx_hash_hex}): {e}"), file=sys.stderr)
                 import traceback
                 traceback.print_exc()
            print("-" * 20) # Separator for events within a block

#+end_src

*Key Changes and Explanations:*

1.  *Asyncio Integration:*
    *   Import =asyncio= and =asgiref.sync.sync_to_async=.
    *   The main =handle= method becomes a simple wrapper that calls =asyncio.run()= on a new =_async_handle= method.
    *   All core logic, including Web3 interaction and the main processing loop, is moved into the =async def _async_handle= method.
    *   The event handler functions (=handle_...=) are now =async def= functions.
    *   =_async_fetch_logs= and =_async_process_events= are also =async def=.

2.  *Async Web3 Provider:*
    *   Initialize =Web3= with a =WebsocketProvider=. This provider supports asynchronous methods.
    *   Web3 methods that perform network calls (like =w3.eth.block_number=, =contract.events.YourEvent.get_logs=, =w3.eth.get_block=) must now be =await=ed.

3.  *Asynchronous ORM Access (=sync_to_async=):*
    *   The standard Django ORM is synchronous. To use it within =async= functions, you must wrap calls with =sync_to_async()=.
    *   Helper functions like =async_get_or_create_user=, =async_create_notification=, etc., are created to wrap common ORM patterns.
    *   Crucially, database /write/ operations (like =.create()=, =.save()=, =.update_or_create()=) and transactions (=transaction.atomic=) should ideally use =sync_to_async(..., thread_sensitive=True)=. This tells =asgiref= to run the sync code in the same dedicated worker thread for that async task, which is often necessary for Django ORM's thread safety assumptions, especially within transactions.
    *   Database /read/ operations can sometimes use =thread_sensitive=False= if you're confident about thread safety, but it's safer to use =True= for consistency.
    *   Transaction blocks must be wrapped: =@sync_to_async(thread_sensitive=True) @transaction.atomic=. The function decorated must contain the synchronous ORM calls.

4.  *Fetching Events (=_async_fetch_logs=):*
    *   Replaced the simpler =get_all_entries= call with fetching logs per event type using =contract.events.YourEvent.get_logs=. This is the standard async way in =web3.py= and allows for more targeted fetching using topics.
    *   Collects logs from both contracts within the specified block range.

5.  *Processing Loop:*
    *   The main loop in =_async_handle= now checks =await sync_to_async(w3.eth.block_number)()=.
    *   It fetches all logs for the blocks since the last processed one up to the current latest block using =_async_fetch_logs=. This handles catching up on blocks that arrived while the indexer was processing the previous batch or was down.
    *   It calls the async =_async_process_events= and =_async_process_block_events= to handle the fetched events.

6.  *Saving State:*
    *   =async_save_last_processed_block= is used to asynchronously save the last processed block number to the database /after/ a batch of events (or a full block) has been successfully processed.

7.  *Error Handling and Polling:*
    *   =asyncio.sleep= is used for waiting between polling loops, allowing the async event loop to manage other tasks.
    *   Includes basic =try...except= blocks for network errors and processing errors, with delays before retrying.

*To Use This:*

1.  *Install =asgiref=:* =pip install asgiref= (often already installed with Django).
2.  *Update Models:* Ensure your =Event=, =Ticket=, and =LastProcessedBlock= models have the necessary fields as discussed. Run migrations.
3.  *Update Settings:* Configure the WebSocket URL (=WEB3_PROVIDER_WS=) and other paths/addresses in your =settings.py=.
4.  *Place ABIs:* Ensure contract ABIs are in the specified JSON files.
5.  *Run:* =python manage.py index_contracts=

This async polling approach is a good step towards a more responsive indexer within the constraints of a management command. For high-volume, mission-critical indexing, moving to a dedicated service outside the Django web process might be necessary for better resource isolation, monitoring, and handling of complex scenarios like chain reorganizations.

*** Keep it simple without sync_to_async. Use queue to publish events and process them in a separate sync thread

Okay, let's structure this using a simple queue and a separate thread for synchronous database processing, avoiding =asyncio= and =sync_to_async= in the event handlers.

This approach keeps the Web3 interaction (polling for logs) in the main thread and offloads the potentially slower database writes to a dedicated background thread.

#+begin_src python
# app_name/management/commands/index_contracts.py

import time
import json
import os
import sys
import queue
import threading
from datetime import datetime
from django.core.management.base import BaseCommand, CommandError
from django.db import transaction, IntegrityError
from django.conf import settings
from django.contrib.auth import get_user_model
from web3 import Web3
from web3.exceptions import ContractLogicError, TransactionNotFound, BlockNotFound

# Assuming your models are in an app named 'core'
try:
    # Add contract_id, cancel_date fields to Event model
    # Add ipfs_cid field to Ticket model
    # Define the LastProcessedBlock model as shown previously (the simple sync version is fine)
    from core.models import CyberValleyUser, EventPlace, Event, Ticket, Notification, LastProcessedBlock
except ImportError as e:
    raise CommandError(f"Could not import models from core. Make sure core.models.py exists and the app is in INSTALLED_APPS. Error: {e}")

# Install ipfs-cid library: pip install ipfs-cid multicodec
from cid import make_cid
import multicodec

User = get_user_model()

# Special queue item to signal the processing thread to stop
STOP_SIGNAL = object()

# --- Helper function to encode Multihash components to CID ---
def multihash_to_cid_v1_raw(digest_bytes: bytes, hash_function_code: int, size: int) -> str | None:
    """Encodes multihash components into an IPFS CIDv1 string (raw codec)."""
    try:
        try:
            hash_codec_info = multicodec.get_codec(code=hash_function_code)
            hash_func_name = hash_codec_info['name']
        except KeyError:
             print(f"Unknown hash function code: {hash_function_code}", file=sys.stderr)
             return None

        try:
             cid_codec_info = multicodec.get_codec(code=0x55) # 0x55 is 'raw'
             cid_codec_name = cid_codec_info['name']
        except KeyError:
            print("Raw multicodec (0x55) not found?", file=sys.stderr)
            return None

        if len(digest_bytes) < size:
            print(f"Digest bytes length ({len(digest_bytes)}) is less than specified size ({size}).", file=sys.stderr)
            return None
        hash_value_bytes = digest_bytes[:size]

        cid = make_cid(1, codec=cid_codec_name, hash_function=hash_func_name, hash_value=hash_value_bytes)
        return str(cid)
    except Exception as e:
        print(f"Error encoding CID from multihash components: {e}", file=sys.stderr)
        return None

# --- Synchronous Database Interaction Functions ---
# These functions are designed to run in the processing thread.
# They use the standard synchronous Django ORM.
# Using transaction.atomic is important here for data consistency.

def get_or_create_user(address: str) -> CyberValleyUser:
    """Gets or creates a CyberValleyUser based on address."""
    checksum_address = Web3.to_checksum_address(address)
    # get_or_create is atomic for creation, but we'll wrap in command handler transactions
    user, created = User.objects.get_or_create(address=checksum_address)
    return user

def create_notification(user: CyberValleyUser, title: str, body: str) -> None:
    """Creates a Notification for a user."""
    if user:
        try:
            # Wrap in atomic if creating multiple related objects, but a single create is usually fine
            Notification.objects.create(user=user, title=title, body=body)
            # print(f"Created notification for {user.address}: {title}") # Too verbose in logs
        except Exception as e:
            print(f"Error creating notification for {user.address}: {e}", file=sys.stderr)


# --- Synchronous Event Handler Functions ---
# These handlers process event data received from the queue.
# They perform database operations directly.

def handle_event_place_available(event_data: dict):
    args = event_data['args']
    event_place_id = args['eventPlaceId']
    # block_timestamp = event_data['block']['timestamp'] # Access block data if passed

    try:
        with transaction.atomic():
            if EventPlace.objects.filter(id=event_place_id).exists():
                print(f"EventPlace {event_place_id} already exists on NewEventPlaceAvailable event. Skipping creation.", file=sys.stderr)
                return

            event_place = EventPlace.objects.create(
                id=event_place_id,
                title=f"Event Place {event_place_id}", # Placeholder title
                max_tickets=args['maxTickets'],
                min_tickets=args['minTickets'],
                min_price=args['minPrice'],
                min_days=args['minDays'],
                available=True,
            )
            print(f"Indexed NewEventPlaceAvailable: Place ID {event_place.id}")

    except IntegrityError as e:
         print(f"Integrity Error handling NewEventPlaceAvailable (Place ID {event_place_id}): {e}", file=sys.stderr)
    except Exception as e:
        print(f"Error handling NewEventPlaceAvailable (Place ID {event_place_id}): {e}", file=sys.stderr)


def handle_event_place_updated(event_data: dict):
    args = event_data['args']
    event_place_id = args['eventPlaceId']
    # block_timestamp = event_data['block']['timestamp']

    try:
        with transaction.atomic():
            event_place = EventPlace.objects.get(id=event_place_id)
            event_place.max_tickets = args['maxTickets']
            event_place.min_tickets = args['minTickets']
            event_place.min_price = args['minPrice']
            event_place.min_days = args['minDays']
            event_place.save()
            print(f"Indexed EventPlaceUpdated: Place ID {event_place.id}")
        except EventPlace.DoesNotExist:
             print(f"Error: EventPlace {event_place_id} not found for update!", file=sys.stderr)
        except Exception as e:
            print(f"Error handling EventPlaceUpdated (Place ID {event_place_id}): {e}", file=sys.stderr)


def handle_new_event_request(event_data: dict):
    args = event_data['args']
    event_contract_id = args['id']
    # block_timestamp = event_data['block']['timestamp']

    try:
        # Assume Event model has contract_id = models.PositiveIntegerField(unique=True)
        with transaction.atomic():
            if Event.objects.filter(contract_id=event_contract_id).exists():
                print(f"Event with contract_id {event_contract_id} already exists on NewEventRequest event. Skipping creation.", file=sys.stderr)
                return

            creator_user = get_or_create_user(args['creator'])
            try:
                event_place = EventPlace.objects.get(id=args['eventPlaceId'])
            except EventPlace.DoesNotExist:
                 print(f"Error: EventPlace {args['eventPlaceId']} not found for NewEventRequest (Event ID {event_contract_id})!", file=sys.stderr)
                 return

            status_map = {0: "submitted", 1: "approved", 2: "declined", 3: "cancelled", 4: "closed"}
            initial_status = status_map.get(args.get('initialStatus', 0), "submitted")

            event_obj = Event.objects.create(
                 contract_id=event_contract_id,
                 creator=creator_user,
                 place=event_place,
                 ticket_price=args['ticketPrice'],
                 start_date=datetime.fromtimestamp(args['startDate']),
                 days_amount=args['daysAmount'],
                 status=initial_status,
                 title=f"Event {event_contract_id}", # Placeholder
                 description="No description available from contract event.", # Placeholder
                 image_url=None, # Placeholder
                 tickets_bought=0,
                 # created_at=datetime.fromtimestamp(block_timestamp), # Use DB default or remove
                 # updated_at=datetime.fromtimestamp(block_timestamp), # Use DB default or remove
                 cancel_date=datetime.fromtimestamp(args['cancelDate']),
            )

            print(f"Indexed NewEventRequest: Event ID {event_contract_id}")

            # Notification for Master
            master_address = getattr(settings, 'MASTER_ADDRESS', None)
            if master_address:
                 master_user = get_or_create_user(master_address)
                 create_notification(
                      master_user,
                      "New Event Request",
                      f"A new event request (ID: {event_contract_id}) has been submitted by {args['creator']} for place {args['eventPlaceId']}."
                 )

    except IntegrityError as e:
         print(f"Integrity Error handling NewEventRequest (Event ID {event_contract_id}): {e}", file=sys.stderr)
    except Exception as e:
        print(f"Error handling NewEventRequest (Event ID {event_contract_id}): {e}", file=sys.stderr)


def handle_event_status_changed(event_data: dict):
    args = event_data['args']
    event_contract_id = args['eventId']
    # block_timestamp = event_data['block']['timestamp']

    try:
        with transaction.atomic():
            event_obj = Event.objects.select_related('creator').get(contract_id=event_contract_id)

            status_map = {
                0: "submitted", 1: "approved", 2: "declined",
                3: "cancelled", 4: "closed",
            }
            new_status_str = status_map.get(args['status'], "unknown")

            if new_status_str == "unknown":
                 print(f"Error: Unknown event status {args['status']} for event {event_contract_id}", file=sys.stderr)
                 return

            event_obj.status = new_status_str
            # event_obj.updated_at = datetime.fromtimestamp(block_timestamp) # Use DB default
            event_obj.save()

            print(f"Indexed EventStatusChanged: Event ID {event_contract_id}, Status {new_status_str}")

            # Send notifications based on status change
            creator_user = event_obj.creator
            event_title = event_obj.title or f"Event {event_obj.contract_id}"

            if new_status_str == "approved":
                create_notification(
                    creator_user,
                    "Event Approved",
                    f"Your event '{event_title}' has been approved!"
                )

            elif new_status_str == "declined":
                create_notification(
                    creator_user,
                    "Event Declined",
                    f"Your event '{event_title}' has been declined."
                )

            elif new_status_str == "cancelled":
                create_notification(
                     creator_user,
                     "Event Cancelled",
                     f"Your event '{event_title}' has been cancelled."
                )
                # Fetch tickets within the atomic block for consistency
                for ticket in event_obj.tickets.select_related('owner').all():
                    create_notification(
                        ticket.owner,
                        "Event Cancelled",
                        f"The event '{event_title}' you bought a ticket for has been cancelled."
                    )

            elif new_status_str == "closed":
                create_notification(
                     creator_user,
                     "Event Closed",
                     f"Your event '{event_title}' has been closed."
                )
                # Fetch tickets within the atomic block
                for ticket in event_obj.tickets.select_related('owner').all():
                    create_notification(
                        ticket.owner,
                        "Event Closed",
                        f"The event '{event_title}' you bought a ticket for has been closed."
                    )

    except Event.DoesNotExist:
         print(f"Error: Event {event_contract_id} not found for status change!", file=sys.stderr)
    except Exception as e:
        print(f"Error handling EventStatusChanged (Event ID {event_contract_id}): {e}", file=sys.stderr)


def handle_event_updated(event_data: dict):
    args = event_data['args']
    event_contract_id = args['id']
    # block_timestamp = event_data['block']['timestamp']

    try:
        with transaction.atomic():
            event_obj = Event.objects.get(contract_id=event_contract_id)
            try:
                 event_place = EventPlace.objects.get(id=args['eventPlaceId'])
            except EventPlace.DoesNotExist:
                 print(f"Error: EventPlace {args['eventPlaceId']} not found for EventUpdated (Event ID {event_contract_id})!", file=sys.stderr)
                 return

            event_obj.place = event_place
            event_obj.ticket_price = args['ticketPrice']
            event_obj.start_date = datetime.fromtimestamp(args['startDate'])
            event_obj.days_amount = args['daysAmount']
            event_obj.cancel_date = datetime.fromtimestamp(args['cancelDate'])
            # event_obj.updated_at = datetime.fromtimestamp(block_timestamp)

            event_obj.save()

            print(f"Indexed EventUpdated: Event ID {event_contract_id}")

    except Event.DoesNotExist:
         print(f"Error: Event {event_contract_id} not found for update!", file=sys.stderr)
    except Exception as e:
        print(f"Error handling EventUpdated (Event ID {event_contract_id}): {e}", file=sys.stderr)


def handle_ticket_minted(event_data: dict):
    args = event_data['args']
    event_contract_id = args['eventId']
    ticket_token_id = args['ticketId']
    # block_timestamp = event_data['block']['timestamp']

    ticket_id_str = str(ticket_token_id)

    try:
        with transaction.atomic():
            if Ticket.objects.filter(id=ticket_id_str).exists():
                 print(f"Ticket with ID {ticket_id_str} already exists on TicketMinted event. Skipping creation.", file=sys.stderr)
                 return

            owner_user = get_or_create_user(args['owner'])
            try:
                event_obj = Event.objects.select_related('creator').get(contract_id=event_contract_id)
            except Event.DoesNotExist:
                 print(f"Error: Event {event_contract_id} not found for TicketMinted (Ticket ID {ticket_id_str})!", file=sys.stderr)
                 return

            ipfs_cid_str = multihash_to_cid_v1_raw(args['digest'], args['hashFunction'], args['size'])
            if ipfs_cid_str is None:
                 print(f"Could not encode CID for ticket {ticket_id_str}.", file=sys.stderr)

            ticket = Ticket.objects.create(
                 id=ticket_id_str,
                 event=event_obj,
                 owner=owner_user,
                 is_redeemed=False,
                 ipfs_cid=ipfs_cid_str,
             )

            event_obj.tickets_bought += 1
            event_obj.save()

            print(f"Indexed TicketMinted: Ticket ID {ticket.id} for Event {event_obj.contract_id}")

            # Send notifications
            event_title = event_obj.title or f"Event {event_obj.contract_id}"
            create_notification(
                 owner_user,
                 "Ticket Purchased",
                 f"You have successfully purchased ticket {ticket.id} for event '{event_title}'."
            )
            create_notification(
                 event_obj.creator,
                 "Ticket Purchased",
                 f"A ticket has been purchased for your event '{event_title}' by {owner_user.address} (Ticket ID: {ticket.id})."
            )

    except IntegrityError as e:
        print(f"Integrity Error handling TicketMinted (Ticket ID {ticket_id_str}): {e}", file=sys.stderr)
    except Exception as e:
        print(f"Error handling TicketMinted (Ticket ID {ticket_id_str}): {e}", file=sys.stderr)


def handle_ticket_redeemed(event_data: dict):
    args = event_data['args']
    ticket_token_id = args['ticketId']

    ticket_id_str = str(ticket_token_id)

    try:
        with transaction.atomic():
            # Use select_related to fetch owner and event
            ticket = Ticket.objects.select_related('owner', 'event__creator').get(id=ticket_id_str)

            if ticket.is_redeemed:
                 print(f"Ticket {ticket.id} was already redeemed.", file=sys.stderr)
                 return

            ticket.is_redeemed = True
            ticket.save()

            print(f"Indexed TicketRedeemed: Ticket ID {ticket.id}")

            # Send notifications
            event_obj = ticket.event
            event_title = event_obj.title or f"Event {event_obj.contract_id}"
            master_address = getattr(settings, 'MASTER_ADDRESS', None)
            master_user = get_or_create_user(master_address) if master_address else None

            create_notification(
                 ticket.owner,
                 "Ticket Redeemed",
                 f"Your ticket ({ticket.id}) for event '{event_title}' has been redeemed."
            )
            create_notification(
                 event_obj.creator,
                 "Ticket Redeemed",
                 f"A ticket ({ticket.id}) for your event '{event_title}' has been redeemed."
            )
            if master_user:
                 create_notification(
                      master_user,
                      "Ticket Redeemed",
                      f"A ticket ({ticket.id}) for event '{event_title}' has been redeemed by staff."
                 )

    except Ticket.DoesNotExist:
         print(f"Error: Ticket {ticket_id_str} not found for redemption!", file=sys.stderr)
    except Exception as e:
        print(f"Error handling TicketRedeemed (Ticket ID {ticket_id_str}): {e}", file=sys.stderr)

# TODO: Implement synchronous handlers for RoleGranted and RoleRevoked

# Mapping event names to their synchronous handler functions
EVENT_HANDLERS = {
    'NewEventPlaceAvailable': handle_event_place_available,
    'EventPlaceUpdated': handle_event_place_updated,
    'NewEventRequest': handle_new_event_request,
    'EventStatusChanged': handle_event_status_changed,
    'EventUpdated': handle_event_updated,
    'TicketMinted': handle_ticket_minted,
    'TicketRedeemed': handle_ticket_redeemed,
    # 'RoleGranted': handle_role_granted,
    # 'RoleRevoked': handle_role_revoked,
}

# --- Queue Processing Thread Function ---
def process_queue(event_queue: queue.Queue):
    """Worker function for the processing thread."""
    print("Event processing thread started.")
    # Ensure Django database connections are handled correctly in threads
    # In simple cases, Django's default connection management might be sufficient,
    # but for long-running threads, explicitly managing connections is safer.
    # With Django 3.1+, =close_old_connections= is often enough when polled.
    # For older versions or more complex setups, consider =connections['default'].connect()= and =close()=.
    from django.db import connections
    def close_connections():
        connections['default'].close_if_unusable_or_obsolete()

    while True:
        try:
            item = event_queue.get()
            if item is STOP_SIGNAL:
                print("Event processing thread received stop signal. Exiting.")
                break

            event_data = item # The item is expected to be the event dict
            event_name = event_data.get('event')
            tx_hash_hex = event_data.get('transactionHash', b'').hex()
            log_index = event_data.get('logIndex', 'N/A')
            block_num = event_data.get('blockNumber', 'N/A')

            if not event_name:
                 print(f"Skipping invalid item from queue (no event name): {item}", file=sys.stderr)
                 event_queue.task_done()
                 continue

            print(f"[Worker] Processing event: {event_name} (Block: {block_num}, Tx: {tx_hash_hex}, Log Index: {log_index})")

            handler = EVENT_HANDLERS.get(event_name)
            if handler:
                try:
                    # Ensure database connections are healthy before processing
                    close_connections()
                    handler(event_data)
                except Exception as e:
                    # Log the error but continue processing other events
                    print(f"[Worker] Error handling event {event_name} (Tx: {tx_hash_hex}): {e}", file=sys.stderr)
                    import traceback
                    traceback.print_exc()
            else:
                print(f"[Worker] No handler found for event: {event_name}", file=sys.stderr)

            event_queue.task_done() # Mark the task as done in the queue

        except queue.Empty:
            # Should not happen with simple get(), but good practice
            print("[Worker] Queue empty, waiting...")
            time.sleep(0.1)
        except Exception as e:
            # Catch unexpected errors in the worker loop
            print(f"[Worker] An unexpected error occurred: {e}", file=sys.stderr)
            import traceback
            traceback.print_exc()
            # Decide how to handle this - logging and continuing is usually okay,
            # but frequent errors might indicate a bigger problem.
            time.sleep(1) # Prevent tight loop on persistent errors

    print("Event processing thread finished.")

# --- Django Management Command ---
class Command(BaseCommand):
    help = 'Listens to smart contract events, queues them, and processes data in a separate thread.'

    def add_arguments(self, parser):
        parser.add_argument(
            '--from-block',
            type=int,
            help='Start syncing from this block number (overrides last saved block).',
            default=None
        )
        parser.add_argument(
            '--no-sync',
            action='store_true',
            help='Skip historical sync and only listen for new events from the latest block.',
        )


    def get_last_processed_block(self, start_block_override: int | None) -> int:
         """Gets the last processed block number from DB or uses override."""
         if start_block_override is not None:
              self.stdout.write(f"Using --from-block override: {start_block_override}")
              # Save override to DB for consistency
              LastProcessedBlock.objects.update_or_create(pk=1, defaults={'block_number': start_block_override - 1}) # Start sync from override block
              return start_block_override - 1 # Return the block *before* starting
         try:
             last_block_entry = LastProcessedBlock.objects.get_or_create(pk=1)[0]
             self.stdout.write(f"Last processed block from DB: {last_block_entry.block_number}")
             return last_block_entry.block_number
         except Exception as e:
              self.stdout.write(self.style.ERROR(f"Error retrieving last processed block from DB: {e}. Starting from block 0."))
              # Fallback to 0 on database error
              LastProcessedBlock.objects.update_or_create(pk=1, defaults={'block_number': 0})
              return 0

    def save_last_processed_block(self, block_number: int) -> None:
         """Saves the last processed block number to DB."""
         if block_number is None:
             return # Don't save None
         try:
              # Using update_or_create for thread safety if multiple indexers ran briefly
              LastProcessedBlock.objects.update_or_create(pk=1, defaults={'block_number': block_number})
              # print(f"Saved last processed block: {block_number}") # Too verbose
         except Exception as e:
              self.stdout.write(self.style.ERROR(f"Error saving last processed block {block_number} to DB: {e}"))


    def handle(self, *args, **options):
        # Use HTTPProvider for simplicity and sync operation
        # Replace with WSProvider if needed, but main loop will need to manage async calls blocking
        # For pure sync, HTTP is best.
        provider_url = getattr(settings, 'WEB3_PROVIDER_HTTP', None)
        if not provider_url:
             # Fallback to WS if only WS is configured, but warn
             provider_url = getattr(settings, 'WEB3_PROVIDER_WS', None)
             if provider_url:
                 self.stdout.write(self.style.WARNING("Using WSProvider in sync mode. May have unexpected blocking behavior."))
                 w3_provider = Web3.WebsocketProvider(provider_url, websocket_timeout=60)
             else:
                  raise CommandError("Web3 provider URL (WEB3_PROVIDER_HTTP or WEB3_PROVIDER_WS) must be configured in settings.py.")
        else:
            w3_provider = Web3.HTTPProvider(provider_url, request_kwargs={'timeout': 60})


        em_address = getattr(settings, 'EVENT_MANAGER_ADDRESS', None)
        et_address = getattr(settings, 'EVENT_TICKET_ADDRESS', None)
        em_abi_path = getattr(settings, 'EVENT_MANAGER_ABI_PATH', None)
        et_abi_path = getattr(settings, 'ET_ABI_PATH', None) # Check your setting name
        poll_interval = getattr(settings, 'INDEXER_POLL_INTERVAL', 5) # Seconds to wait between checking for new blocks
        error_retry_delay = getattr(settings, 'INDEXER_ERROR_RETRY_DELAY', 15) # Seconds to wait after an error
        batch_size = getattr(settings, 'INDEXER_FETCH_BATCH_SIZE', 1000) # Number of blocks to fetch logs for at a time

        if not all([em_address, et_address, em_abi_path, et_abi_path]):
             raise CommandError("Contract addresses (EVENT_MANAGER_ADDRESS, EVENT_TICKET_ADDRESS), and ABI paths (EVENT_MANAGER_ABI_PATH, ET_ABI_PATH) must be configured in settings.py.")

        try:
            w3 = Web3(w3_provider)

            if not w3.is_connected():
                raise CommandError(f"Could not connect to Web3 provider at {provider_url}")

            self.stdout.write(self.style.SUCCESS(f"Connected to Web3 provider at {provider_url}"))

            try:
                em_abi = get_contract_abi(em_abi_path)
                et_abi = get_contract_abi(et_abi_path)
            except FileNotFoundError as e:
                raise CommandError(f"ABI file not found: {e}")
            except ValueError as e:
                 raise CommandError(f"Invalid ABI file: {e}")

            event_manager_contract = w3.eth.contract(address=Web3.to_checksum_address(em_address), abi=em_abi)
            event_ticket_contract = w3.eth.contract(address=Web3.to_checksum_address(et_address), abi=et_abi)

            self.stdout.write(self.style.SUCCESS("Contract objects created."))

            # Set up the event queue and processing thread
            event_queue = queue.Queue()
            processor_thread = threading.Thread(target=process_queue, args=(event_queue,))
            processor_thread.daemon = True # Allow thread to exit if main program exits
            processor_thread.start()
            self.stdout.write(self.style.SUCCESS("Event processing thread started."))

            # --- Historical Sync ---
            last_processed_block = self.get_last_processed_block(options['from_block'])
            current_block = w3.eth.block_number

            if options['no_sync']:
                 self.stdout.write("Historical sync skipped (--no-sync flag).")
                 last_checked_block = current_block # Start live polling from current block
                 self.save_last_processed_block(last_checked_block) # Update DB
            elif current_block > last_processed_block:
                self.stdout.write(f"Starting historical sync from block {last_processed_block + 1} to {current_block}")
                sync_start_block = last_processed_block + 1
                sync_end_block = current_block

                # Fetch and queue historical events in batches
                processed_block_during_sync = last_processed_block # Track progress during sync
                while sync_start_block <= sync_end_block:
                    batch_end_block = min(sync_start_block + batch_size - 1, sync_end_block)
                    self.stdout.write(f"Fetching historical events from block {sync_start_block} to {batch_end_block}...")
                    try:
                        # Use getLogs with topics for efficiency if possible
                        # This requires defining topics for each event type you care about
                        # Or fetch all logs and parse (less efficient but simpler)
                        # Let's fetch all logs for the contracts' addresses and let the worker parse

                        # Get topics for all event types
                        em_event_topics = [
                            event_manager_contract.events.NewEventPlaceAvailable().topics[0],
                            event_manager_contract.events.EventPlaceUpdated().topics[0],
                            event_manager_contract.events.NewEventRequest().topics[0],
                            event_manager_contract.events.EventStatusChanged().topics[0],
                            event_manager_contract.events.EventUpdated().topics[0],
                            # Add RoleGranted/RoleRevoked topics
                        ]
                        et_event_topics = [
                            event_ticket_contract.events.TicketMinted().topics[0],
                            event_ticket_contract.events.TicketRedeemed().topics[0],
                        ]
                        all_topics = em_event_topics + et_event_topics


                        logs = w3.eth.get_logs({
                            'address': [event_manager_contract.address, event_ticket_contract.address],
                            'fromBlock': sync_start_block,
                            'toBlock': batch_end_block,
                            'topics': [all_topics] # Filter by any of these topics
                        })

                        self.stdout.write(f"Fetched {len(logs)} logs.")

                        # Process/decode logs and queue them
                        processed_count = 0
                        for log in logs:
                            try:
                                # Try to parse the log using ABIs
                                event_data = None
                                if log['address'] == event_manager_contract.address:
                                     event_data = event_manager_contract.events.get_event_by_signature(log['topics'][0].hex()).process_log(log)
                                elif log['address'] == event_ticket_contract.address:
                                     event_data = event_ticket_contract.events.get_event_by_signature(log['topics'][0].hex()).process_log(log)

                                if event_data:
                                    # Add block data if needed by handlers
                                    # Fetching block data for every event during sync can be slow
                                    # Only add if timestamp or other block info is critical in handler
                                    # event_data['block'] = w3.eth.get_block(log['blockNumber']) # This is slow!
                                    event_queue.put(event_data)
                                    processed_count += 1
                                else:
                                     self.stdout.write(self.style.WARNING(f"Could not decode log from block {log['blockNumber']}, tx {log['transactionHash'].hex()}"))

                            except Exception as e:
                                self.stdout.write(self.style.ERROR(f"Error decoding/queuing log from block {log.get('blockNumber')}: {e}"))
                                import traceback
                                traceback.print_exc()


                        self.stdout.write(f"Queued {processed_count} historical events from block {sync_start_block} to {batch_end_block}.")

                        # Update processed block marker *after* queueing the batch
                        processed_block_during_sync = batch_end_block
                        self.save_last_processed_block(processed_block_during_sync)

                        sync_start_block = batch_end_block + 1 # Move to the next batch

                    except Exception as e:
                        self.stdout.write(self.style.ERROR(f"Error fetching historical logs from {sync_start_block} to {batch_end_block}: {e}. Retrying batch after delay."))
                        import traceback
                        traceback.print_exc()
                        time.sleep(error_retry_delay) # Wait before retrying the same batch
                        # Do NOT update processed_block_during_sync here, retry starts from the same block

                # Historical sync finished
                self.stdout.write(self.style.SUCCESS(f"Historical sync finished up to block {processed_block_during_sync}"))
                last_checked_block = processed_block_during_sync # Start live polling from here

            else:
                self.stdout.write(f"Database is already up-to-date (last processed block: {last_processed_block}). Starting live polling.")
                last_checked_block = current_block # Ensure we poll from latest block if DB is current
                self.save_last_processed_block(last_checked_block) # Update DB just in case


            # --- Live Polling ---
            self.stdout.write(f"Starting live event indexing (polling every {poll_interval} seconds from block {last_checked_block + 1})...")

            while True:
                try:
                    latest_block = w3.eth.block_number

                    from_block_live = last_checked_block + 1
                    to_block_live = latest_block

                    if to_block_live >= from_block_live: # Only fetch if there are new blocks
                        self.stdout.write(f"Checking blocks {from_block_live} to {to_block_live} for events...")
                        try:
                            # Fetch new logs, again using get_logs with topics
                            em_event_topics = [e().topics[0] for e in [event_manager_contract.events.NewEventPlaceAvailable, event_manager_contract.events.EventPlaceUpdated, event_manager_contract.events.NewEventRequest, event_manager_contract.events.EventStatusChanged, event_manager_contract.events.EventUpdated]]
                            et_event_topics = [e().topics[0] for e in [event_ticket_contract.events.TicketMinted, event_ticket_contract.events.TicketRedeemed]]
                            all_topics = em_event_topics + et_event_topics


                            logs = w3.eth.get_logs({
                                'address': [event_manager_contract.address, event_ticket_contract.address],
                                'fromBlock': from_block_live,
                                'toBlock': to_block_live,
                                'topics': [all_topics]
                            })

                            self.stdout.write(f"Found {len(logs)} new logs.")

                            # Queue new events
                            processed_count = 0
                            for log in logs:
                                try:
                                    event_data = None
                                    if log['address'] == event_manager_contract.address:
                                         event_data = event_manager_contract.events.get_event_by_signature(log['topics'][0].hex()).process_log(log)
                                    elif log['address'] == event_ticket_contract.address:
                                         event_data = event_ticket_contract.events.get_event_by_signature(log['topics'][0].hex()).process_log(log)

                                    if event_data:
                                         # Queue the event data
                                         event_queue.put(event_data)
                                         processed_count += 1
                                    else:
                                         self.stdout.write(self.style.WARNING(f"Could not decode new log from block {log['blockNumber']}, tx {log['transactionHash'].hex()}"))

                                except Exception as e:
                                     self.stdout.write(self.style.ERROR(f"Error decoding/queuing new log from block {log.get('blockNumber')}: {e}"))
                                     import traceback
                                     traceback.print_exc()


                            if processed_count > 0:
                                self.stdout.write(f"Queued {processed_count} new events from blocks {from_block_live} to {to_block_live}.")

                            # Update last checked block after successfully fetching logs for the range
                            # The actual processing happens in the worker thread
                            last_checked_block = to_block_live
                            self.save_last_processed_block(last_checked_block)
                            self.stdout.write(f"Marked up to block {last_checked_block} as checked.")

                        except Exception as e:
                             self.stdout.write(self.style.ERROR(f"Error fetching new logs from {from_block_live} to {to_block_live}: {e}. Retrying fetch range."))
                             import traceback
                             traceback.print_exc()
                             # Do not update last_checked_block, will retry fetching the same range
                             time.sleep(error_retry_delay)
                             continue # Skip wait below and retry fetch loop

                    else:
                         # No new blocks, just wait
                         pass

                    # Wait before checking for new blocks again
                    time.sleep(poll_interval)

                except KeyboardInterrupt:
                    self.stdout.write(self.style.NOTICE("\nIndexer stopping..."))
                    break
                except Exception as e:
                    self.stdout.write(self.style.ERROR(f"An unexpected error occurred in the polling loop: {e}"))
                    import traceback
                    traceback.print_exc()
                    self.stdout.write(f"Retrying after {error_retry_delay} seconds...")
                    time.sleep(error_retry_delay)

            # --- Shutdown ---
            self.stdout.write("Putting stop signal in queue...")
            event_queue.put(STOP_SIGNAL) # Signal the worker thread to stop
            # Optional: Join the thread to wait for it to finish processing remaining items
            # processor_thread.join()
            # However, for a management command that exits, joining might block indefinitely
            # if there's an issue in the worker. Allowing daemon thread exit might be preferred.
            # A clean shutdown would wait for the queue to be empty before putting STOP_SIGNAL
            # and then joining.

            # Simple shutdown: let daemon thread exit
            self.stdout.write("Indexer main thread finished.")


        except CommandError as e:
            self.stdout.write(self.style.ERROR(e))
        except Exception as e:
            self.stdout.write(self.style.ERROR(f"An error occurred during setup or initial sync: {e}"))
            import traceback
            traceback.print_exc()

# --- Helper function to get ABI ---
def get_contract_abi(abi_path: str) -> list:
    """Loads ABI from a JSON file."""
    full_abi_path = os.path.join(settings.BASE_DIR, abi_path)
    if not os.path.exists(full_abi_path):
        raise FileNotFoundError(f"ABI file not found at: {full_abi_path}")
    with open(full_abi_path, 'r') as f:
        try:
            abi_data = json.load(f)
            abi = abi_data.get('abi', abi_data)
            if not isinstance(abi, list):
                 raise ValueError("Invalid ABI format: Expected a list or artifact JSON.")
            return abi
        except json.JSONDecodeError:
             raise ValueError(f"Invalid JSON in ABI file: {full_abi_path}")
        except Exception as e:
             raise ValueError(f"Error loading ABI from {full_abi_path}: {e}")

#+end_src

/Key Changes and Concepts:/

1.  *Synchronous Web3:* The code uses =web3.py` with `HTTPProvider` which is blocking/synchronous. If you *must* use a WebSocket provider, you might need to wrap the Web3 calls in a way that handles the underlying async nature, or switch back to an `asyncio` based main loop, but the goal here was to remove `sync_to_async`. `HTTPProvider` is the simplest for a sync polling loop.
2.  **`queue.Queue`:** A standard Python thread-safe queue is used (`event_queue`). The main thread puts event data into the queue, and the processing thread gets data from it.
3.  **Processing Thread:**
    *   A function `process_queue` is defined to run as the target for a `threading.Thread`.
    *   This thread continuously calls `event_queue.get()`. `get()` blocks until an item is available, so the thread sleeps efficiently when the queue is empty.
    *   The thread runs standard synchronous code, including Django ORM calls.
    *   `thread.daemon = True` means the thread will exit automatically when the main process exits (e.g., on `KeyboardInterrupt`).
    *   Includes logic to call `connections['default'].close_if_unusable_or_obsolete()` periodically or before DB access in the worker thread. This is good practice for long-running threads interacting with Django's ORM to prevent stale connections.
4.  **Event Handlers:**
    *   The `handle_...` functions are back to being regular synchronous Python functions.
    *   They take the raw `event_data` dictionary as input.
    *   They perform Django ORM operations directly within `transaction.atomic()` blocks for consistency.
5.  **Polling Loop:**
    *   The main `handle` method connects to Web3, performs historical sync by fetching logs in batches, and then enters a `while True` loop.
    *   Inside the loop, it polls `w3.eth.block_number` to find new blocks.
    *   If new blocks exist, it fetches logs for the new block range using `w3.eth.get_logs` (filtered by contract addresses and event topics for efficiency).
    *   Fetched logs are decoded using `contract.events.get_event_by_signature(...).process_log(...)`. This step is important because `get_logs` returns raw log entries, not the processed event objects you get from `get_all_entries`.
    *   Decoded event data is put into the `event_queue`.
    *   The `last_checked_block` is updated only after successfully fetching logs for a block range.
6.  **Shutdown:**
    *   On `KeyboardInterrupt`, the main loop breaks.
    *   A special `STOP_SIGNAL` object is put into the queue.
    *   The `process_queue` function checks for this signal and exits its loop when it receives it.
7.  **Log Decoding:** Added explicit log decoding using `contract.events.get_event_by_signature().process_log()`. This is necessary when using `w3.eth.get_logs` which returns raw logs. Event topics are generated and used for efficient filtering in `get_logs`.

*To Use This Version:*

1.  **Update Models:** Ensure `Event`, `Ticket`, `LastProcessedBlock` models are correctly defined (same as the first sync version). Run migrations.
2.  **Update Settings:** Configure `WEB3_PROVIDER_HTTP` (recommended) or `WEB3_PROVIDER_WS`, `EVENT_MANAGER_ADDRESS`, `EVENT_TICKET_ADDRESS`, `EVENT_MANAGER_ABI_PATH`, `ET_ABI_PATH`, and optionally `MASTER_ADDRESS`, `INDEXER_POLL_INTERVAL`, `INDEXER_ERROR_RETRY_DELAY`, `INDEXER_FETCH_BATCH_SIZE`.
3.  **Place ABIs:** Ensure the `.json` ABI files are at the paths specified.
4.  **Install Dependencies:** `pip install web3 ipfs-cid multicodec`.
5.  **Run Command:** Execute with `python manage.py index_contracts`.

This version successfully separates the network interaction from the database persistence, keeping the database code purely synchronous while polling for events in the main thread and using a queue for communication.

***
